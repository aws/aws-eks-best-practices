# Architecting for static stability

When an availability zone outage occurs, the best way to maintain application availability is to be prepared. Rely on Kubernetes’ static stability, rather than automated evacuation strategies. Configure applications with [pod anti affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) and [topology spread constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints) to spread replicas across failure domains, and test to verify that replicas can handle the traffic surge during an outage.

Kubernetes nodes have a periodic heartbeat every 40 seconds. This informs the Kubernetes control plane that the node is healthy, able to accept new pods from the scheduler, and continue to execute existing pods bound to the node. There are a number of cases where the node heartbeat will stop, including network partitioning, hardware failure, or shutdown of the underlying machine. These events may be isolated to a single node, or correlated across many nodes in the cluster.

For an isolated availability event, pods should be terminated and rescheduled elsewhere to restore the desired available replicas of its owning controller. Kubernetes achieves this through [taint based eviction](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions). After 40 seconds without a heartbeat, the node lifecycle controller applies a NoExecute taint to the node. By default, pods tolerate this NoExecute taint for 300 seconds, before they are terminated by the pod lifecycle controller. The owning controller launches a new pod, which is then scheduled to a healthy node.

This behavior works well for isolated availability events, but what happens during a correlated event, such as an availability zone or regional power failure? When 55% of the nodes in an availability zone fail to send a heartbeat, the node lifecycle controller determines that the cluster is in a state of partial disruption and halts taint based eviction. Once the number of available nodes returns below this threshold, taint based eviction is restored. If an entire availability zone goes down, why doesn’t Kubernetes to taint all of the nodes and migrate the workloads to another availability zone? Architecting for availability zone evacuation (e.g. running at a high utilization with the plan to launch new capacity during an AZ failure) contains multiple bad assumptions that risk application availability.

First, is the assumption that capacity will be available during the outage. If the event is correlated, workloads across accounts may be impacted. If each of these workloads evacuate at the same time, this will result in a large spike in demand for capacity in the other availability zones. In many cases, this can be absorbed by large capacity pools in EC2, but risks increase for higher demand instance types and smaller regions.

Second, is the assumption that control plane orchestration systems are not impacted by the correlated outage. While these systems are highly available, the number of systems involved in failover spans many AWS services, core Kubernetes components, and user installed Kubernetes operators. Evacuation results in a large spike in traffic across all of these systems, and correlated impact to any one of these systems can cause an evacuation strategy to fail.

Third, is the introduction of bi-modal system behavior. Availability zone evacuation is a rare event, outside of the standard operations of the system. This mechanism is expensive to test, and thus, rarely tested. Even if it has been tested, changes to the system over time may violate original assumptions, which won’t be discovered until it’s too late.

Static stability solves this problem, and is a recommended best practice in both AWS and Kubernetes. Application replicas should be spread across common failure domains, including across virtual machines and availability zones. In the event of a correlated outage, the automation halts changes, allowing the existing capacity to handle the workload’s traffic. Networking layers shift traffic away from the impacted availability zone towards the healthy availability zones. Application scale should be tuned according to the reliability requirements of the application. To survive a single availability zone outage, replicas in other zones must be preemptively scaled to handle the additional load. Workload performance can be nonlinear, so it’s essential to test. To survive a multiple availability zone event, additional overhead is required. At a fundamental level, redundancy is traded for cost.

For Kubernetes applications, this is best achieved using [pod anti affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) and [topology spread constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints). Concretely, Kubernetes deployments should be configured with anti affinity for the key kubernetes.io/hostname and topology spread constraints for the key topology.kubernetes.io/zone. Together, these will ensure that the deployment’s pods are spread across availability zones and nodes. The workload should be both vertically and horizontally sized, using resource requests and replica count respectively. [AWS Fault Injection Service](https://docs.aws.amazon.com/fis/latest/userguide/fis-actions-reference.html#eks-actions-reference) can help to test these configurations against a simulated availability event and validate utilization assumptions when traffic shifts away from the impacted zone.
