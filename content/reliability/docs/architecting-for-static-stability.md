# Architecting for static stability

When an Availability Zone outage occurs, the best way to maintain application availability is to be prepared. Static stability is an architectural pattern that enables systems to maintain overall availablity when a part of the system becomes unavailable. Kubernetes enables users to build statically stable applications with [pod anti affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) and [topology spread constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints). Configure these settings to spread application replicas across failure domains, and test to verify that replicas can handle the traffic surge during an outage.

Kubernetes nodes have a periodic heartbeat every 40 seconds. This informs the Kubernetes control plane that the node is healthy, able to accept new pods from the scheduler, and continue to execute existing pods bound to the node. There are a number of cases where the node heartbeat will stop, including network partitioning, hardware failure, or shutdown of the underlying machine. These events may be isolated to a single node, or correlated across many nodes in the cluster.

For an isolated availability event, pods should be terminated and rescheduled elsewhere to restore the desired available replicas of its owning controller. Kubernetes achieves this through [taint based eviction](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions). After 40 seconds without a heartbeat, the node lifecycle controller applies a NoExecute taint to the node. By default, pods tolerate this NoExecute taint for 300 seconds, before they are terminated by the pod lifecycle controller. The owning controller launches a new pod, which is then scheduled to a healthy node.

This behavior works well for isolated availability events, but what happens during a correlated event, such as an Availability Zone or regional power failure? When 55% of the nodes in an Availability Zone fail to send a heartbeat, the node lifecycle controller determines that the cluster is in a state of partial disruption and halts taint based eviction. Once the number of unavailable nodes falls below this threshold, taint based eviction is restored. If an entire Availability Zone goes down, why doesn’t Kubernetes taint all of the nodes and migrate the workloads to another Availability Zone? Architecting for Availability Zone evacuation (e.g. running at a high utilization with the plan to launch new capacity during an AZ failure) contains multiple bad assumptions that risk application availability.

First is the assumption that capacity will be available in other Availability Zones during the outage. Correlated events can affect other customer workloads as well. If every workload evacuates simultaneously, it will cause a large spike in demand for capacity in the other Availability Zones. Most times, large capacity pools in EC2 will absorb the demand, but risks increase for higher demand instance types and smaller regions.

Second is the assumption that the correlated outage will not impact control plane orchestration systems. While these systems are designed to be highly available, the number of systems involved in a failover scenario spans multiple AWS services, Kubernetes core components, and user installed Kubernetes operators. Evacuation results in a large spike in traffic across these systems, and correlated impact on any of these systems can cause an evacuation strategy to fail.

Third is the introduction of bi-modal system behavior. Availability Zone evacuation is a rare event and introduces a new operational mode in addition to the standard operational mode of the system. This mechanism is expensive to test, and thus, rarely tested. Changes to the system over time may violate original assumptions and test results, the impact of which won’t surface until it’s too late.

Static stability solves this problem and is a recommended best practice in both AWS and Kubernetes. Consider spreading application replicas across multiple failure domains, including across virtual machines and Availability Zones. In the event of a correlated outage, the automation halts changes, allowing the existing capacity to handle the workload’s traffic. Networking layers shift traffic away from the impacted Availability Zone towards the replicas in healthy Availability Zones. Application scaling should correspond to its reliability requirements. To survive a single Availability Zone outage, replicas in other AZs must be preemptively scaled to handle the additional load. Workload performance can be nonlinear, so it’s essential to test. To survive a multiple availability zone event, larger overhead is required. At a fundamental level, customers must choose the right balance of redundancy and cost that meets their use case.

For Kubernetes applications, this is best achieved using [pod anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) and [topology spread constraints](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints). Consider using anti-affinity for the key `kubernetes.io/hostname` and topology spread constraints for the key `topology.kubernetes.io/zone` in Deployments. Together, these will ensure that the Kubernetes scheduler spreads the Deployment’s Pods across multiple Availability Zones and nodes. The workload should be both vertically and horizontally sized, using resource requests and replica count, respectively. [AWS Fault Injection Service](https://docs.aws.amazon.com/fis/latest/userguide/fis-actions-reference.html#eks-actions-reference) can help with testing these configurations against a simulated availability event and validate utilization assumptions when traffic shifts away from the impacted zone.
