# Security Pillar
The Security pillar includes the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies.

## Definition
There are five best practice areas for Security in the cloud:

+ Identity and access management 
+ Detective controls
+ Infrastructure protection
+ Data protection
+ Incident response

Before you architect any system, you need to think about its security implications and the practices that can affect your security posture. For example, you need to control who can perform actions against a set of resources. You also need the ability to quickly identify security incidents, protect your systems and services from unauthorized access, and maintain the confidentiality and integrity of data through data protection. Having a well-defined and rehearsed process for responding to security incidents will help too. These tools and techniques are important because they support objectives such as preventing financial loss or complying with regulatory obligations.

AWS helps organizations achieve their security and compliance goals by offering a rich set of security services that have evolved based on feedback from a broad set of security conscious customers. By offering a highly secure foundation, customers can spend less time on “undifferentiated heavy lifting” and more time on achieving their business objectives. 

## Best Practices
### Identity and Access Management
Identity and Access Management (IAM) is an AWS service that performs 2 essential functions: Authentication and Authorization.  Authentication involves the verification of a indentity whereas authorization governs the actions that can be performed by AWS resources.  Within AWS, a resource can be another AWS service, e.g. EC2, or an AWS principle such as an IAM User or Role.  The rules governing the actions that a resource is allowed to peform are expressed as IAM policies.  

### Controlling Access to EKS Clusters
The Kubernetes project supports a variety of different strategies to authenticate requests to the kube-apiserver service, e.g. Bearer Tokens, X.509 certificates, OIDC, etc. EKS currently has native support for [webhook token authentication](https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication) and [serivce account tokens](https://kubernetes.io/docs/reference/access-authn-authz/authentication/#service-account-tokens).  

The webhook authentication strategy calls a webhook that verifies bearer tokens. On EKS, these bearer tokens are generated by the AWS CLI or the [aws-iam-authenticator](https://github.com/kubernetes-sigs/aws-iam-authenticator) client when you run `kubectl` commands. As you execute commands, the token is passed to the kube-apiserver which forwards it to the authentication webhook.  If the request is well-formed, the webhook calls a pre-signed URL embedded in the token's body. This URL validates the request's signature and returns information about the user, e.g. the user's account, Arn, and UserId to the kube-apiserver.  

To generate a authentication token, type the following command in a terminal window: 
```
aws eks get-token --cluster <cluster_name>
```
The output should resemble this: 
```json
{"kind": "ExecCredential", "apiVersion": "client.authentication.k8s.io/v1alpha1", "spec": {}, "status": {"expirationTimestamp": "2020-02-19T16:08:27Z", "token": "k8s-aws-v1.aHR0cHM6Ly9zdHMuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFKTkdSSUxLTlNSQzJXNVFBJTJGMjAyMDAyMTklMkZ1cy1lYXN0LTElMkZzdHMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIwMDIxOVQxNTU0MjdaJlgtQW16LUV4cGlyZXM9NjAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JTNCeC1rOHMtYXdzLWlkJlgtQW16LVNpZ25hdHVyZT0yMjBmOGYzNTg1ZTMyMGRkYjVlNjgzYTVjOWE0MDUzMDFhZDc2NTQ2ZjI0ZjI4MTExZmRhZDA5Y2Y2NDhhMzkz"}}
```
Each token starts with `k8s-aws-v1.` followed by a base64 encoded string. The string, when decoded, should resemble this: 
```
https://sts.amazonaws.com/?Action=GetCallerIdentity&Version=2011-06-15&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJNGRILKNSRC2W5QA%2F20200219%2Fus-east-1%2Fsts%2Faws4_request&X-Amz-Date=20200219T155427Z&X-Amz-Expires=60&X-Amz-SignedHeaders=host%3Bx-k8s-aws-id&X-Amz-Signature=220f8f3585e320ddb5e683a5c9a405301ad76546f24f28111fdad09cf648a393
```
The token consists of a pre-signed URL that includes an Amazon credential and signature. For additional see https://docs.aws.amazon.com/STS/latest/APIReference/API_GetCallerIdentity.html. 

The token has a time to live (TTL) of 15 minutes after which a new token will need to be generated. This is handled automatically when you use a client like `kubectl` but if you're using the Kubernetes dashboard, you will need to generate a new token and re-authenticate every 15 minutes. 

Once the user's identity has been authenticated by the AWS IAM service, the kube-apiserver reads the `aws-auth` ConfigMap in the kube-system namespace to determine the RBAC group to associate with the user.  The `aws-auth` ConfigMap is used to create a static mapping betweeen IAM principles, i.e. IAM Users and Roles, and Kubernetes RBAC groups. RBAC groups can be referenced in Kubernetes RoleBindings or ClusterRoleBindings. They are similar to IAM Roles in that they define a set of actions (verbs) that can be peformed against a collection of Kubernetes resources (objects).

#### Recommendations
+ **Don't use a service account token for authentication**. A service account token is a long-lived credential. If it is compromised, lost, or stolen, an attacker may be able to perform all the actions associated with that token until the service account is deleted. You may need to grant an exception for applications that have to consume the Kubernetes API from outside the cluster, e.g. a CI/CD pipeline application. If such applications run on AWS infrastructure, like EC2 instances, consider using an instance profile and mapping that to a Kubernetes RBAC role in the aws-auth ConfigMap instead.  

+ **Employ least privileged access to AWS Resources**. An IAM User does not need to be assigned privileges to AWS resources to access the Kubernetes API. If you need to grant an IAM user access to an EKS cluster, create an entry in the `aws-auth` ConfigMap for that user that maps to a specific Kubernetes RBAC group. 

+ **Use IAM Roles when multiple users need identical access to the cluster**. Rather than creating an entry for each individual IAM User in the aws-auth ConfigMap, allow those users to assume an IAM Role and map that role to a Kubernetes RBAC group.  This will be easier to maintain, especially as the number of users that require access grows.

+ **Employ least privilieged access when creating RoleBindings and ClusterRoleBindings**. Like the earlier point about granting access to AWS Resources, RoleBindings and ClusterRoleBindings should only include the set of permissions necessary to perform a specific function. Avoid using `["*"]` in your Roles and ClusterRoles unless it's absolutely necessary. If you're unsure what permissions to assign, consider using a tool like [audit2rbac](https://github.com/liggitt/audit2rbac) to automatically generate Roles and binding based on the observed API calls in the Kubernetes Audit Log.

+ **Regularly audit access to the cluster**. Who requires access is likely to change over time. Periodically review the aws-auth ConfigMap to see who has been granted access and the rights they've been assigned. You can also use open source tooling like [kubectl-who-can](https://github.com/aquasecurity/kubectl-who-can), or [rbac-lookup](https://github.com/FairwindsOps/rbac-lookup) to examine the roles bound to a particular service account, user, or group. We'll explore this topic further when we get to the section on auditing.  Additional ideas can be found in this [article](https://www.nccgroup.trust/us/about-us/newsroom-and-events/blog/2019/august/tools-and-methods-for-auditing-kubernetes-rbac-policies/?mkt_tok=eyJpIjoiWWpGa056SXlNV1E0WWpRNSIsInQiOiJBT1hyUTRHYkg1TGxBV0hTZnRibDAyRUZ0VzBxbndnRzNGbTAxZzI0WmFHckJJbWlKdE5WWDdUQlBrYVZpMnNuTFJ1R3hacVYrRCsxYWQ2RTRcL2pMN1BtRVA1ZFZcL0NtaEtIUDdZV3pENzNLcE1zWGVwUndEXC9Pb2tmSERcL1pUaGUifQ%3D%3D) from nccgroup. 

+ **Make the EKS Cluster Endpoint private**. By default when you provision an EKS cluster, the API cluster endpoint is set to public, i.e. it can be accessed from the Internet. Despite being public, the endpoint is considered secure because it requires that all API requests are first authenticated and then authorized. That said, if your security policy mandates that you restrict access to the API from the Internet or prevents you from routing traffic outside the cluster VPC, you can: 
    + Configure the EKS cluster endpoint to be private. See [Modifying Cluster Endpoint Access](https://docs.aws.amazon.com/eks/latest/userguide/cluster-endpoint.html) for further information on this topic. 
    + Leave the cluster endpoint public and specify which CIDR blocks can communicate with the cluster endpoint. The blocks are effectively a whitelisted set of public IP addresses that are allowed to access the cluster endpoint.
    + Configure public access with a set of whitelisted CIDR blocks and set private endpoint access to enabled. This will allow public access from a specific range of public IPs while forcing all network traffic between the kubelets (workers) and the Kubernetes API through the cross-account ENIs that get provisioned into the cluster VPC when the control plane is provisioned.

#### Alternative Approaches to Authentication and Access Management
While IAM is the preferred way to authenticate users who need access to an EKS cluster, it is possible to use an OIDC identity provider such as GitHub using an authentication proxy and Kubernetes [impersonation](https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation). Posts for 2 such solutions have been published on the AWS Open Source blog:
+ [Authenticating to EKS Using GitHub Credentials with Teleport](https://aws.amazon.com/blogs/opensource/authenticating-eks-github-credentials-teleport/)
+ [Consistent OIDC authentication across multiple EKS clusters using kube-oidc-proxy](https://aws.amazon.com/blogs/opensource/consistent-oidc-authentication-across-multiple-eks-clusters-using-kube-oidc-proxy/)

You can also use [AWS SSO](https://docs.aws.amazon.com/singlesignon/latest/userguide/what-is.html) to federate AWS with an external identity provider, e.g. Azure AD. If you decide to use this, the AWS CLI v2.0 includes an option to create a named profile that makes it easy to associate an SSO session with your current CLI session and assume an IAM role. Know that you must assume a role prior to running `kubectl` as the IAM role is used to determine the user's Kubernetes RBAC group.

### Pods Identities
Certain applications that run within a Kubernetes cluster need permission to call the Kubernetes API to function properly. For example, the ALB Ingress Controller needs to be able to list a service's endpoints. The controller also needs to be able to invoke AWS APIs to provision and configure an ALB.  In this section we will explore the best practices for assigning rights and privileges to pods. 

#### Kubernetes Service Accounts
A service account is a special type of object that allows you to assign a Kubernetes RBAC role to a pod.  A default service account is created for each namespace within a cluster. When you deploy a pod into a namespace without referencing a specific service account, the default service account for that namespace will automatically get assigned to the pod and the secret, i.e. the service account (JWT) token for that service account, will get mounted to the pod as a volume at `/var/run/secrets/kubernetes.io/serviceaccount`. Decoding the service account token in that directory will reveal the following metadata: 
```json
{
  "iss": "kubernetes/serviceaccount",
  "kubernetes.io/serviceaccount/namespace": "default",
  "kubernetes.io/serviceaccount/secret.name": "default-token-5pv4z",
  "kubernetes.io/serviceaccount/service-account.name": "default",
  "kubernetes.io/serviceaccount/service-account.uid": "3b36ddb5-438c-11ea-9438-063a49b60fba",
  "sub": "system:serviceaccount:default:default"
}
``` 

The default service account has the following permissions to the Kubernetes API.
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2020-01-30T18:13:25Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:discovery
  resourceVersion: "43"
  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/system%3Adiscovery
  uid: 350d2ab8-438c-11ea-9438-063a49b60fba
rules:
- nonResourceURLs:
  - /api
  - /api/*
  - /apis
  - /apis/*
  - /healthz
  - /openapi
  - /openapi/*
  - /version
  - /version/
  verbs:
  - get
```
This role authorizes unauthenticated and authenticated users to read API information and is deemed safe to be publicly accessible.

When an application running within a pod calls the Kubernetes APIs, the pod needs to be assigned a service account that grants it permission to do so.  Similar to guidelines for user access, the Role or ClusterRole bound to a service account should be restricted to the API resources and methods that the application needs to function and nothing else. To use a non-default service account simply set the `spec.serviceAccountName` field of a pod to the name of the service account you wish to use. For additional information about creating service accounts, see https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions. 

#### IAM Roles for Service Accounts (IRSA)
IRSA is a new feature that allows you to assign an IAM role to a Kubernetes service account. It works by leveraging a Kubernetes feature known as [Service Account Token Volume Projection](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection). Pods with service accounts that reference an IAM Role call a public OIDC discovery endpoint for AWS IAM upon startup. The endpoint cyrptographically signs the OIDC token issued by Kubernetes which ultimately allows the pod to call the AWS APIs associated IAM role. When an AWS API is invoked, the AWS SDKs calls `sts:AssumeRoleWithWebIdentity` and automatically exchanges the Kubernetes issued token for a AWS role credential. 

Decoding the (JWT) token for IRSA will produce output similar to the example you see below: 
```json
{
  "aud": [
    "sts.amazonaws.com"
  ],
  "exp": 1582306514,
  "iat": 1582220114,
  "iss": "https://oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865987144EA99A26FB128",
  "kubernetes.io": {
    "namespace": "default",
    "pod": {
      "name": "alpine-57b5664646-rf966",
      "uid": "5a20f883-5407-11ea-a85c-0e62b7a4a436"
    },
    "serviceaccount": {
      "name": "s3-read-only",
      "uid": "a720ba5c-5406-11ea-9438-063a49b60fba"
    }
  },
  "nbf": 1582220114,
  "sub": "system:serviceaccount:default:s3-read-only"
}
```
This particular token grants the pod view-only privileges to S3. When the application attempt to read from S3, the token is exchanged for a temporary set of IAM credentials that resembles this: 
```json
{
    "AssumedRoleUser": {
        "AssumedRoleId": "AROA36C6WWEJULFUYMPB6:abc", 
        "Arn": "arn:aws:sts::820537372947:assumed-role/eksctl-winterfell-addon-iamserviceaccount-de-Role1-1D61LT75JH3MB/abc"
    }, 
    "Audience": "sts.amazonaws.com", 
    "Provider": "arn:aws:iam::820537372947:oidc-provider/oidc.eks.us-west-2.amazonaws.com/id/D43CF17C27A865987144EA99A26FB128", 
    "SubjectFromWebIdentityToken": "system:serviceaccount:default:s3-read-only", 
    "Credentials": {
        "SecretAccessKey": "ORJ+8Adk+wW+nU8FETq7+mOqeA8Z6jlPihnV8hX1", 
        "SessionToken": "FwoGZXIvYXdzEGMaDMLxAZkuLpmSwYXShiL9A1S0X87VBC1mHCrRe/pB2oes+l1eXxUYnPJyC9ayOoXMvqXQsomq0xs6OqZ3vaa5Iw1HIyA4Cv1suLaOCoU3hNvOIJ6C94H1vU0siQYk7DIq9Av5RZe+uE2FnOctNBvYLd3i0IZo1ajjc00yRK3v24VRq9nQpoPLuqyH2jzlhCEjXuPScPbi5KEVs9fNcOTtgzbVf7IG2gNiwNs5aCpN4Bv/Zv2A6zp5xGz9cWj2f0aD9v66vX4bexOs5t/YYhwuwAvkkJPSIGvxja0xRThnceHyFHKtj0H+bi/PWAtlI8YJcDX69cM30JAHDdQH+ltm/4scFptW1hlvMaP+WReCAaCrsHrAT+yka7ttw5YlUyvZ8EPog+j6fwHlxmrXM9h1BqdikomyJU00gm1++FJelfP+1zAwcyrxCnbRl3ARFrAt8hIlrT6Vyu8WvWtLxcI8KcLcJQb/LgkW+sCTGlYcY8z3zkigJMbYn07ewTL5Ss7LazTJJa758I7PZan/v3xQHd5DEc5WBneiV3iOznDFgup0VAMkIviVjVCkszaPSVEdK2NU7jtrh6Jfm7bU/3P6ZG+CkyDLIa8MBn9KPXeJd/y+jTk5Ii+fIwO/+mDpGNUribg6TPxhzZ8b/XdZO1kS1gVgqjXyVC+M+BRBh6C4H21w/eMzjCtDIpoxt5rGKL6Nu/IFMipoC4fgx6LIIHwtGYMG7SWQi7OsMAkiwZRg0n68/RqWgLzBt/4pfjSRYuk=", 
        "Expiration": "2020-02-20T18:49:50Z", 
        "AccessKeyId": "ASIA36C6WWEJUMHA3L7Z"
    }
}
```  

A mutating webhook that runs as part of the EKS control plane injects the AWS Role Arn and the path to a web identity token file into the pod as environment variables. These values can also be supplied manually. 
```
AWS_ROLE_ARN=arn:aws:iam::AWS_ACCOUNT_ID:role/IAM_ROLE_NAME
AWS_WEB_IDENTITY_TOKEN_FILE=/var/run/secrets/eks.amazonaws.com/serviceaccount/token
```

The kubelet will automatically rotate the projected token when it is older than 80% of its total TTL, or after 24 hours. The AWS SDKs are responsible for reloading the token when it rotates. For further information about IRSA, see https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-technical-overview.html.

#### Recommendations
+ **Disable auto-mounting of service account tokens**. If your application doesn't need to call the Kubernetes API set the `automountServiceAccountToken` attribute to `false` in the PodSec for your application or patch the default service account in each namespace so that it's no longer mounted to pods automatically. For example: 
    ```
    kubectl patch serviceaccount default -p $'automountServiceAccountToken: false'
    ```
+ **Use dedicated service accounts for each application**. Each application should have its own dedicated service account.  This applies to service accounts for the Kubernetes API as well as IRSA. 

    If you employ a blue/green approach to cluster upgrades instead of performing an in-place cluster upgrade, you will need to update the trust policy of each of the IRSA IAM roles with the OIDC endpoint of the new cluster. A blue/green cluster upgrade is where you create a cluster running a newer version of Kubernetes alongside the old cluster and use a load balancer or a service mesh to seamlessly shift traffic from services running on the old cluster to the new cluster. 

+ **Restrict access to the instance profile assigned to the worker node**. When you use IRSA, the pod no longer inherits the rights of the instance profile assigned to the worker node. Nonetheless, as an added precaution, you may want to block a process's ability to access EC2 metadata. This will effectively prevent pods that do not use IRSA from inheriting the role assigned to the worker node. Be aware that when you block access to EC2 metadata on a worker node, it may prevent certain pods from functioning properly. For additional information about how to block access to instance metadata, see https://docs.aws.amazon.com/eks/latest/userguide/restrict-ec2-credential-access.html.

+ **Run the application as a non-root user**. Containers run as root by default. While this allows them to to read the web identity token file, running a container as root is not considered a best practice. As an alternative, consider adding the `spec.securityContext.runAsUser` attribute to the PodSpec.  The value of `runAsUser` is abritrary value.   

+ **Scope the IAM Role trust policy for IRSA to the service account name**. The trust policy can be scoped to a namespace or a specific service account within a namespace. When using IRSA it's best to make the role trust policy as explicit as possible by including the service account name. This will effectively prevent other pods within the same namespace from assuming the role. The CLI `eksctl` will do this automatically when you use it to create service accounts/IAM roles. See https://eksctl.io/usage/iamserviceaccounts/ for futher information. 

#### Alternative approaches
While IRSA is the _preferred way_ to assign an AWS "identity" to a pod, it requires that you include recent version of the AWS SDKs in your application. For a complete listing of the SDKs that currently support IRSA, see https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts-minimum-sdk.html. If you have an application that you can't immediately update with a IRSA-compatible SDK, there are several community-built solutions available for assigning IAM roles to Kubernetes pods, including kube2iam, kiam, and iam4kube.  Although AWS doesn't endorse or condone the use of these solutions, they are frequently used by the community at large to achieve similar results as IRSA. 

## Pod Security

Pods have variety of different settings that can strengthen or weaken your overall security posture.  As a Kubernetes practitioner your chief concern should be preventing a process that’s running in a container from escaping the isolation boundaries of Docker and gaining access to the underlying host.  The reason for this is twofold.  First, the processes that run within a container run under the context of the \[Linux\] root by default.  Although the actions of root within a container are constrained by the set of Linux capabilities that Docker assigns to the containers, these default privileges could allow an attacker to escalate their privileges and/or gain access to sensitive information bound to the host, including Secrets and ConfigMaps.  Below is a list of default capabilities assigned to Docker containers.  For additional information about each capability, see http://man7.org/linux/man-pages/man7/capabilities.7.html.

`CAP_CHOWN, CAP_DAC_OVERERIDE, CAP_FOWNER, CAP_FSETID, CAP_KILL, CAP_SETGID, CAP_SETUID, CAP_SETPCAP, CAP_NET_BIND_SERVICE, CAP_NET_RAW, CAP_SYS_CHROOT, CAP_MKNOD, CAP_AUDIT_WRITE, CAP_SETFCAP`

Pods that are run as privileged, inherit _all_ of the Linux capabilities associated with root on the host and should be avoided if possible.

Second, all Kubernetes worker nodes use an authorization mode called the node authorizer.  The node authorizer authorizes all API requests that originate from the kubelet and allows nodes to perform the following actions: 

Read operations:

+ services
+ endpoints
+ nodes
+ pods
+ secrets, configmaps, persistent volume claims and persistent volumes related to pods bound to the kubelet’s node

Write operations:

+ nodes and node status (enable the `NodeRestriction` admission plugin to limit a kubelet to modify its own node)
+ pods and pod status (enable the `NodeRestriction` admission plugin to limit a kubelet to modify pods bound to itself)
+ events

Auth-related operations:

+ read/write access to the certificationsigningrequests API for TLS bootstrapping
+ the ability to create tokenreviews and subjectaccessreviews for delegated authentication/authorization checks

EKS uses the [node restriction admission controller](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction) which only allows the node to modify a limited set of node attributes and pod objects that are bound to the node.   Nevertheless, an attacker who manages to get access to the host will be able to glean sensitive information about the environment from the Kubernetes API that could allow them to move laterally within the cluster.

### Recommendations

+ **Restrict the containers that can run as privileged**.  As mentioned, containers that run as privileged inherit all of the Linux capabilities assigned to root on the host.  Seldom do containers need these types of privileges to function properly.  You can reject pods with containers configured to run as privileged by creating a [pod security policy](https://kubernetes.io/docs/concepts/policy/pod-security-policy/).  You can think of a pod security policy as a set requirements that pods have to meet before they can be created.  If you elect to use pod security policies, you will need to create a role binding that allows service accounts to read your pod security policies. 

    When you provision an EKS cluster, a pod security policy called eks.privileged is automatically created.  The manifest for the policy appears below: 

    ```
    kind: PodSecurityPolicy
    apiVersion: policy/v1beta1
    metadata:
      name: eks.privileged
      selfLink: /apis/policy/v1beta1/podsecuritypolicies/eks.privileged
      uid: 36e940ae-438c-11ea-9438-063a49b60fba
      resourceVersion: '188'
      creationTimestamp: '2020-01-30T18:13:28Z'
      labels:
        eks.amazonaws.com/component: pod-security-policy
        kubernetes.io/cluster-service: 'true'
      annotations:
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"policy/v1beta1","kind":"PodSecurityPolicy","metadata":{"annotations":{"kubernetes.io/description":"privileged
          allows full unrestricted access to pod features, as if the
          PodSecurityPolicy controller was not
          enabled.","seccomp.security.alpha.kubernetes.io/allowedProfileNames":"*"},"labels":{"eks.amazonaws.com/component":"pod-security-policy","kubernetes.io/cluster-service":"true"},"name":"eks.privileged"},"spec":{"allowPrivilegeEscalation":true,"allowedCapabilities":["*"],"fsGroup":{"rule":"RunAsAny"},"hostIPC":true,"hostNetwork":true,"hostPID":true,"hostPorts":[{"max":65535,"min":0}],"privileged":true,"readOnlyRootFilesystem":false,"runAsUser":{"rule":"RunAsAny"},"seLinux":{"rule":"RunAsAny"},"supplementalGroups":{"rule":"RunAsAny"},"volumes":["*"]}}
        kubernetes.io/description: >-
          privileged allows full unrestricted access to pod features, as if the
          PodSecurityPolicy controller was not enabled.
        seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
    spec:
      privileged: true
      allowedCapabilities:
        - '*'
      volumes:
        - '*'
      hostNetwork: true
      hostPorts:
        - min: 0
          max: 65535
      hostPID: true
      hostIPC: true
      seLinux:
        rule: RunAsAny
      runAsUser:
        rule: RunAsAny
      supplementalGroups:
        rule: RunAsAny
      fsGroup:
        rule: RunAsAny
      allowPrivilegeEscalation: true
    ```
    This policy has the following role binding: 

    ```
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: 'eks:podsecuritypolicy:authenticated'
      selfLink: >-
        /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/eks%3Apodsecuritypolicy%3Aauthenticated
      uid: 36eb5bd0-438c-11ea-9438-063a49b60fba
      resourceVersion: '190'
      creationTimestamp: '2020-01-30T18:13:28Z'
      labels:
        eks.amazonaws.com/component: pod-security-policy
        kubernetes.io/cluster-service: 'true'
      annotations:
        kubectl.kubernetes.io/last-applied-configuration: >
          {"apiVersion":"rbac.authorization.k8s.io/v1","kind":"ClusterRoleBinding","metadata":{"annotations":{"kubernetes.io/description":"Allow
          all authenticated users to create privileged
          pods."},"labels":{"eks.amazonaws.com/component":"pod-security-policy","kubernetes.io/cluster-service":"true"},"name":"eks:podsecuritypolicy:authenticated"},"roleRef":{"apiGroup":"rbac.authorization.k8s.io","kind":"ClusterRole","name":"eks:podsecuritypolicy:privileged"},"subjects":[{"apiGroup":"rbac.authorization.k8s.io","kind":"Group","name":"system:authenticated"}]}
        kubernetes.io/description: Allow all authenticated users to create privileged pods.
    subjects:
      - kind: Group
        apiGroup: rbac.authorization.k8s.io
        name: 'system:authenticated'
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: 'eks:podsecuritypolicy:privileged'
    ```

    This effectively allows an authenticated user to run privileged containers across all namespaces within the cluster.  While this may seem overly permissive, there are certain applications/plug-ins such as the AWS VPC CNI and kube-proxy that have to run as privileged because they are responsible for configuring the host’s network settings. Furthermore, this policy provides backward compatibility with earlier versions of Kubernetes that lacked support for pod security policies. 

    As a best practice, however, we recommend that you scope the binding for privileged pods to service accounts within a particular namespace, e.g. kube-system, and limiting access to that namespace.  For all other serviceaccounts/namespaces, we recommend implementing a more restrictive policy such as this: 

    ```
    apiVersion: policy/v1beta1
    kind: PodSecurityPolicy
    metadata:
      name: restricted
      annotations:
        seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default'
        apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
        seccomp.security.alpha.kubernetes.io/defaultProfileName:  'runtime/default'
        apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
    spec:
      privileged: false
      # Required to prevent escalations to root.
      allowPrivilegeEscalation: false
      # This is redundant with non-root + disallow privilege escalation,
      # but we can provide it for defense in depth.
      requiredDropCapabilities:
        - ALL
      # Allow core volume types.
      volumes:
        - 'configMap'
        - 'emptyDir'
        - 'projected'
        - 'secret'
        - 'downwardAPI'
        # Assume that persistentVolumes set up by the cluster admin are safe to use.
        - 'persistentVolumeClaim'
      hostNetwork: false
      hostIPC: false
      hostPID: false
      runAsUser:
        # Require the container to run without root privileges.
        rule: 'MustRunAsNonRoot'
      seLinux:
        # This policy assumes the nodes are using AppArmor rather than SELinux.
        rule: 'RunAsAny'
      supplementalGroups:
        rule: 'MustRunAs'
        ranges:
          # Forbid adding the root group.
          - min: 1
            max: 65535
      fsGroup:
        rule: 'MustRunAs'
        ranges:
          # Forbid adding the root group.
          - min: 1
            max: 65535
      readOnlyRootFilesystem: false
    ```

    This policy prevents pods from running as privileged or escalating privileges.  It also restricts the types of volumes that can be mounted and the root supplemental groups that can be added. 
    
    ### Fargate
    Fargate is a launch type that enables you to run "serverless" container(s) where the containers of a pod are run on infrastructure that AWS manages. With Fargate, you cannot run a privileged container or  configure your pod to use hostNetwork or hostPort.  

+ **Do not run processes in containers as root**. All containers run as root by default.  This could be problematic if an attacker is able to exploit a vulnerability in the application and get shell access to the running container.  You can mitigate this risk a variety of ways.  First, by removing the shell from the container image.  Second, adding the USER directive to your Dockerfile or running the containers in the pod as a non-root user.  The Kubernetes podSpec includes a set of fields under spec.securityContext, that allow to let you specify the user and/or group to run your application as.  These fields are `runAsUser` and `runAsGroup` respectively.  You can mandate the use of these fields by creating a pod security policy.  See https://kubernetes.io/docs/concepts/policy/pod-security-policy/#users-and-groups for further information on this topic. 

+ **Restrict the use of hostPath or if hostPath is necessary restrict which prefixes can be used and configure the volume as read-only**. hostPath is a volume that mounts a directory from the host directly to the container.  Rarely will pods need this type of access, but if they do, you need to be aware of the risks.  By default pods that run as root will have write access to the file system exposed by hostPath.  This could allow an attacker to modify the kubelet settings, create symbolic links to directories or files not directly exposed by the hostPath, e.g. /etc/shadow, install an ssh keys, read secrets mounted to the host, and other malicious things. To mitigate the risks from hostPath, configure the spec.containers.volumeMounts as readOnly, for example: 

    ```
    volumeMounts:
    - name: hostPath-volume
      readOnly: true
      mountPath: /host-path
    ```
    You should also use a pod security policy to restrict the directories that can be used by hostPath volumes.  For example the following PSP excerpt only allows paths that begin with /foo.  It will prevent containers from traversing the host file system from outsude the prefix: 
    ```
    allowedHostPaths:
    # This allows "/foo", "/foo/", "/foo/bar" etc., but
    # disallows "/fool", "/etc/foo" etc.
    # "/foo/../" is never valid.
    - pathPrefix: "/foo"
      readOnly: true # only allow read-only mounts
    ```
+ **Set requests and limits for each container to avoid resource contention and DoS attacks**. A pod without requests or limits can theoretically consume all of the resources available on a host.  As additional pods are scheduled onto a node, the node may experience CPU or memory pressure which can cause the Kubelet to terminate or evict pods from the node.  While you can’t prevent this from happening altogether, setting requests and limits will help minimize resource contention and mitigate the risk from poorly written applications that consume an excessive amount of resources. 

    The podSpec allows you to specify requests and limits for CPU and memory.  CPU is considered a compressible resource because it can be oversubscribed.  Memory is incompressible, i.e. it cannot be shared among multiple containers.  

    When you specify requests for CPU or memory, you’re essentially designating the amount of _memory_ that containers are guaranteed to get.  Kubernetes aggregates the requests of all the containers in a pod to determine which node to schedule the pod onto.  If a container exceeds the requested amount of memory it may be subject to termination if there’s memory pressure on the node. 

    Limits are the maximum amount of CPU and memory resources that a container is allowed to consume and directly corresponds to the `memory.limit_in_bytes` value of the cgroup created for the container.  A container that exceeds the memory limit will be OOM killed. If a container exceeds its CPU limit, it will be throttled. 

    Kubernetes uses 3 Quality of Service (QoS) classes to prioritize the workloads running on a node.  These include: guaranteed, burstable, and best effort.  If limits and requests are not set, the pod is configured as burstable (lowest priority).  Burstable pods are the first to get killed when there is insufficient memory.  If limits are set on _all_ containers within the pod, or if the requests and limits are set to the same values, the pod is configured as guaranteed (higheest priority).  Guaranteed pods will not be killed unless they exceed their configured memory limits. If the limits and requests are configured with different values, or 1 container within the pod sets limits and the other don’t or have limits set for different resources, the pods are configured as burstable (medium priority). These pods have some resource guarantees, but can be killed once they exceed their requested memory. Be aware that requests doesn’t actually affect the `memory_limit_in_bytes` value of the cgroup; the cgroup limit is set to the amount of memory available on the host. Nevertheless, setting the requests value too low could cause the pod to be targeted for termination by the kubelet if the node undergoes memory pressure. 

    For additional information about resource QoS, see https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md

    You can force the use of requests and limits by setting a [resource quota](https://kubernetes.io/docs/concepts/policy/resource-quotas/) on a namespace or by creating a [limit range](https://kubernetes.io/docs/concepts/policy/limit-range/).  A resource quota allows you to specify the total amount of resources, e.g. CPU and RAM, allocated to a namespace.  When it’s applied to a namespace, it forces you to specify requests and limits for all containers deployed into that namespace. Limit ranges give you more granular control of the allocation of resources. With limit ranges you can min/max for CPU and memory resources per pod or per container within a namespace.  You can also use them to set default request/limit values if none are provided.  

+ **Do not allow privileged escalation**. Privileged escalation allows a process to change the security context under which its running.  Sudo is a good example of this as are binaries with the SETUID or SETGID flag.  Privileged escalation is basically a way for users to execute a file with the permissions of another user or group.  You can prevent a container from privileged by implementing a pod security policy that sets `allowPriviledgedEscalation` to false or by setting `securityContext.allowPrivilegedEscalation` in the podSpec.  

### Resources
+ [kube-psp-advisor](https://github.com/sysdiglabs/kube-psp-advisor) is a tool that makes it easier to create K8s Pod Security Policies (PSPs) from either a live K8s environment or from a single .yaml file containing a pod specification (Deployment, DaemonSet, Pod, etc).

## Image security
You should consider the container image as your first line of defense against an attack. An insecure, poorly constructed image can allow an attacker to escape the bounds of the container and gain access to the host.  Once on the host, an attacker can gain access to sensitive information or move laterally within the cluster or with your AWS account.  The following best practices will help mitigate risk of this happening. 

+ **Create minimal images**. Start by removing all extraneous binaries from the container image.  If you’re using an unfamiliar image from Dockerhub, inspect the image using an application like Dive (https://github.com/wagoodman/dive) which can show you the contents of each of the container’s layers.  Remove all binaries with the SETUID and SETGID bits as they can be used to escalate privilege and consider removing all shells and utilities like nc and curl that can be used for nefarious purposes. You can find the files with SETUID and SETGID bits with the following command:
    ```
    find / -perm +6000 -type f -exec ls -ld {} \;
    ```
    To remove the special permissions from these files, add the following directive to your container image:
    ```
    RUN find / -xdev -perm +6000 -type f -exec chmod a-s {} \; || true
    ```
    Colloquially, this is known as de-fanging your image. 

+ **Considering signing your images**. When Docker was first introduced, there was no cryptographic model for verifying container images.  With v2, Docker added digests to the image manifest. This allowed an image’s configuration to be hashed and for the hash to be used to generate an ID for the image.  When image signing is enabled, the \[Docker\] engine verifies the manifest’s signature, ensuring that the content was produced from a trusted source and no tampering has occurred. After each layer is downloaded, the engine verifies the digest of the layer, ensuring that the content matches the content specified in the manifest.  Image signing  effectively allows you to create a secure supply chain, through the verification of digital signatures associated with the image. 

  In a Kubernetes environment, you can use an admission controller to verify that an image has been signed, as in these examples: https://github.com/IBM/portieris and https://github.com/kelseyhightower/grafeas-tutorial. By signing your images, you're verifying the publisher (source) ensuring that the image hasn't been tampered with (integrity).
  
  ### Tools
    + [Notary](https://github.com/theupdateframework/notary)
    + [Grafeas](https://grafeas.io/)

+ **Use multi-stage builds**.  Using multi-stage builds is a way to create minimal images. Oftentimes, multi-stage builds are used to automate parts of the Continuous Integration cycle.  For example, multi-stage builds can be used to lint your source code or perform static code analysis.  This affords developers an opportunity to get near immediate feedback instead of waiting for a pipeline to execute.  Multi-stage builds are attractive from a security standpoint because they allow you to minimize the size of the final image pushed to your container registry.  Container images devoid of build tools and other extraneous binaries decreases improves your security posture by reducing the attack surface of the image. For additional information about multi-stage builds, see https://docs.docker.com/develop/develop-images/multistage-build/.

+ **Scan images for vulnerabilities regularly**. Like their VM counterparts, container images can contain binaries and application libraries with vulnerabilities or develop vulnerabilities over time.  The best way to safeguard against exploits is by regularly scanning your images with an image scanner.  Images that are stored in Amazon ECR can be scanned on push or on-demand (once during a 24 hour period). ECR currently leverages Clair (https://github.com/quay/clair) an open source image scanning solution.  After an image is scanned, the results are logged to the event stream for ECR in EventBridge. You can also see the results of a scan from within the ECR console.  Images with a HIGH or CRITICAL vulnerability should be deleted or rebuilt.  If an image that has been deployed develops a vulnerability, it should be replaced as soon as possible. 

    Knowing where images with vulnerabilities have been deployed is essential to keeping your environment secure.  While you could conceivably build an image tracking solution yourself, there are already several commercial offerings that provide this and other advanced capabilities out of the box, including:
    + [Anchore](https://docs.anchore.com/current/)
    + [Twistlock](https://www.twistlock.com/)
    + [Aqua](https://www.aquasec.com/)
    
    A kubernetes validation webhook could also be used to validate that images are free of critical vulnerabilities.  Validation webhooks are invoked prior to the Kubernetes API.  They are typically used to reject requests that don't comply with the validation criteria defined in the webhook.  [This](https://github.com/jicowan/ecr-validation-webhook) is an example of a serverless webhook that calls the ECR describeImageScanFindings API to deteremine whether a pod is pulling an image with critical vulnerabilities.  If vulnerabilities are found, the pod is rejected and a message with list of CVEs is returned as an  event.

+ **Create IAM policies for ECR repositories**. Nowadays, it is not uncommon for an organization to have multiple development teams operating independently within a shared AWS account.  If these teams don't need to share assets, you may want to create a set of IAM policies that restrict access to the repositories each team can interact with.  A good way to implement this is by using namespaces. Namespaces are a way to group similar repositories together.  For example, all of the registries for team A can be prefaced with the team-a/ while those for team B can use the team-b/ prefix. The policy to restrict access might look like the following: 
  ```
  {
    "Version": "2012-10-17",
    "Statement": [{
      "Sid": "AllowPushPull",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:role/<team_a_role_name>"
      },
      "Action": [
        "ecr:GetDownloadUrlForLayer",
        "ecr:BatchGetImage",
        "ecr:BatchCheckLayerAvailability",
        "ecr:PutImage",
        "ecr:InitiateLayerUpload",
        "ecr:UploadLayerPart",
        "ecr:CompleteLayerUpload"
      ],
      "Resource": [
        "arn:aws:ecr:region:123456789012:repository/team-a/*"
      ]
      }]
  }
  ```
+ **Implement endpoint policies for ECR**. The default endpoint policy for allows access to all ECR repositories within a region.  This might allow an attacker/insider to exfiltrate data by packaging it as a container image and pushing it to a registry in another AWS account.  Mitigating this risk involves creating an endpoint policy that limits API access to ECR respositories. For example, the following policy allows all AWS principles in your account to perform all actions against your and only your ECR repositories: 
  ```
  {
    "Statement": [{
        "Sid": "LimitECRAccess",
        "Principal": "*",
        "Action": "*"
        "Effect": "Allow",
        "Resource": "arn:aws:ecr:region:<your_account_id>:repository/*"
      },
    ]
  }
  ```
  We recommended applying the same policy to both the `com.amazonaws.<region>.ecr.dkr` and the `com.amazonaws.<region>.ecr.api` endpoints.

  Note: Since EKS pulls images for kube-proxy, coredns, and aws-node from ECR, you will need to add the account ID of the registry, e.g. `602401143452.dkr.ecr.us-west-2.amazonaws.com/*` to the list of resources in the endpoint policy or alter the policy to allow pulls from "*" and restrict pushes to your account ID.  The table below reveals the mapping between the AWS accounts where EKS images are vended from and cluster region.

  | Account Number | Region |
  | -------------- | ------ |
  | 602401143452 | All commercial regions except for those listed below |
  | 800184023465 | HKG | 
  | 558608220178 | BAH |
  | 918309763551 | BJS | 
  | 961992271922 | ZHY |

+ **Consider using ECR private endpoints**. The ECR API has a public endpoint.  Consequently, ECR registriese can be accessed from the Internet so long as the request has been authenticated and authorized by IAM. For those who need to operate in a sandboxed environment where the cluster VPC lacks an Internet Gateway (IGW), you can configure a private endpoint for ECR.  Creating a private endpoint enables you to privately access the ECR API through a private IP address instead of routing traffic across the Internet. For additional information on this topic, see https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html. 

+ **Create a set of curated images**. Rather than allowing developers to create their own images, consider creating a set of vetted images for the different application stacks in your organization.  By doing so, developers can forego learning how to compose Dockerfiles and concentrate on writing code.  As changes are merged into Master, a CI/CD pipeline can automatically compile the asset, store it in an artifact repository and copy the artifact into the appropriate image before pushing it to a Docker registry like ECR. At the very least you should create a set of base images from which developers to create their own Dockerfiles.  Ideally, you want to avoid pulling images from Dockerhub because a) you don't always know what is in the image and b) about [a fifth](https://www.kennasecurity.com/blog/one-fifth-of-the-most-used-docker-containers-have-at-least-one-critical-vulnerability/) of the top 1000 images have vulnerabilties. A list of those images and their vulnerabilities can be found at https://vulnerablecontainers.org/.

+ **Add the `USER` directive to your Dockerfiles to run as a non-root user.**  As was mentioned in the pod security section, you should avoid running container as root.  While you can configure this as part of the podSpec, it is a good habit to use the `USER` directive to your Dockerfiles.  The `USER` directive sets the UID to use when running `RUN`, `ENTRYPOINT`, or `CMD` instruction that appears after the USER directive.

+ **Lint your Dockerfiles**. Linting can be used to verify that your Dockerfiles are adhering to a set of predefined guidelines, e.g. the inclusion of the `USER` directive or the requirement that all images be tagged.  [dockerfile_lint](https://github.com/projectatomic/dockerfile_lint) is an open source project from RedHat that verifies common best practices and includes a rule engine that you can use to build your own rules for linting Dockerfiles. It can be incorporated into a CI pipeline, in that builds with Dockerfiles that violate a rule will automatically fail. 

+ **Build images from Scratch**. Reducing the attack surface of your container images should be primary aim when building images.  The ideal way to do this is by creating minimal images that are devoid of binaries that can be used to exploit vulnerabilities. Fortunately, Docker has a mechanism to create images from [`scratch`](https://docs.docker.com/develop/develop-images/baseimages/#create-a-simple-parent-image-using-scratch). With langages like Go, you can create a static linked binary and reference it in your Dockerfile as in this example: 
  ```
  ############################
  # STEP 1 build executable binary
  ############################
  FROM golang:alpine AS builder
  # Install git.
  # Git is required for fetching the dependencies.
  RUN apk update && apk add --no-cache git
  WORKDIR $GOPATH/src/mypackage/myapp/
  COPY . .
  # Fetch dependencies.
  # Using go get.
  RUN go get -d -v
  # Build the binary.
  RUN go build -o /go/bin/hello
  ############################
  # STEP 2 build a small image
  ############################
  FROM scratch
  # Copy our static executable.
  COPY --from=builder /go/bin/hello /go/bin/hello
  # Run the hello binary.
  ENTRYPOINT ["/go/bin/hello"]
  ```
  This creates a container image that consists of your application and nothing else, making it extremely secure.  

### Tools
+ [Bane](https://github.com/genuinetools/bane) An AppArmor profile generator for Docker containers
+ [docker-slim](https://github.com/docker-slim/docker-slim) Build secure minimal images
+ [dockerfile-lint](https://github.com/projectatomic/dockerfile_lint) Rule based linter for Dockerfiles
+ [Gatekeeper and OPA](https://github.com/open-policy-agent/gatekeeper) A policy based admission controller

## Tenant Isolation
When we think of multi-tenancy, we often want to isolate a user or application from other users or applications running on a shared infrastructure. Kuberntes is a single tenant orchestrator in that there is an single instance of the API server, controller manager, and scheduler for the whole cluster. You can create the semblance of "tenants" by using various Kubernetes objects, such as namespaces, RBAC and network policies, along with resource quotas or limit ranges.  While these constructs will help to logically isolate tenants from each other and control the amount of cluster resources each tenant can consume, the cluster is still consider the security boundary.  And as described earlier, if an attacker manages to gain access to the underlying host, they could easily retrieve all Secrets, ConfigMaps, and Volumes, mounted on that host.  They could also impersonate the Kubelet which might give them the ability to move laterally within the cluster or manipulate the attributes of the node.  The following sections will explain how to implement tenant isolation while mitigating the risks of using a single tenant orchestrator like Kubernetes.
### Soft multi-tenancy
With soft multi-tenancy, you use native Kubernetes constructs like namespaces, and RBAC and network policies to create logical isolation between tenants.  RBAC roles and bindings help prevent tenants from accessing each other's service accounts and secrets. Resource quotas and limit ranges control the amount of cluster resources each tenant can consume while network policies can help prevent applications deployed into different namespaces from communicating with each other.  This would not, however, prevent pods from different tenants from sharing a node.  If stronger isolation is required, you can use a node selector, anti-affinity rules, and/or taints and tolerations to force pods from different tenants to be scheduled onto separate nodes (sole tenant nodes). This could get rather complicated, and costly, in an environment with lots of tenants.  Additionally, soft multi-tenancy implemented with native Kubernetes objects doesn't give you the flexibility to provide tenants with a filtered list of namespaces or create hierarchical namespaces because namespaces are a globaly scoped Type.   

[Kiosk](https://github.com/kiosk-sh/kiosk) is an open source project that can aid in the implementation of soft multi-tenancy.  It is implemented as a series of CRDs and controllers that provide the following capabilities: 
  + **Accounts & Account Users** to separate tenants in a shared Kubernetes cluster
  + **Self-Service Namespace Provisioning** for account users
  + **Account Limits** to ensure quality of service and fairness when sharing a cluster
  + **Namespace Templates** for secure tenant isolation and self-service namespace initialization

The Kubernetes community has recognized the current shortcomings of soft multi-tenancy and the challenges with hard multi-tenancy. [SIG-Multitenancy](https://github.com/kubernetes-sigs/multi-tenancy) is attempting to address them with several incubation projects, including: hierarchical namespace controller (HNC) and virtual cluster. The HNC proposal (KEP) describes a way to create parent-child relationships between namespaces with \[policy\] object inheritance along with an ability for tenant administrators to create subnamespaces. Virtual Cluster proposal describes a mechanism for creating separate instances of the control plane services, e.g. the api server, the controller manager, and scheduler, for each tenant within the cluster (kubernetes on kubernetes).

There are 3 primary use cases that can be addressed by soft multi-tenancy.  The first is in an Enterprise setting where the "tenants" are semi-trusted in that they are employees of the organization.  Each tenant will typically align to a department or team. A cluster administrator will usually be responsible for creating namespaces, managing policies like quotas, and/or implementing a delegated adminstration model where certain individuals are given partial oversight of a namespace, i.e. CRUD operations for non-policy related objects like pods, jobs, etc. The isolation provided by Docker may be acceptable within this setting or it may need to be augmented with additional controls such as Pod Security Policies (PSPs). It may also be necessary to restrict communication between services in different namespaces if stricter isolation is required.  

By constrast, soft multi-tenancy can be used in settings where want to offer Kubernetes as a service (KaaS).  With KaaS, your application is hosted in a shared cluster along with a collection of controllers and CRDs that provide a set of PaaS services.  Tenants interact directly with the Kubernetes API server and are permitted to perform CRUD operations on non policy objects.  There is also an element of self-service in that tenants may be allowed to create and manage their own namespaces.  In this type of environment, tenants are assumed to be running untrusted code.  As such, you'll need to implement strict network policies as well as sandboxing.  Sandboxing is where you run the containers of a pod inside a micro VM like Firecracker or in a user-space kernel.  Outside of Fargate, EKS doesn't support this type of configuration.  

The final use case for soft multi-tenancy is in a SaaS setting where you're deploying an instance of an application into a namespace for each tenant.  Unlike the other use cases, the tenant does not directly interface with the Kubernetes API.  Instead, users go through a proxy to get provision an instance of the application on the cluster.  The proxy is responsible for sending the necessary commands to the  Kubernetes API server to create a new namespace and provision a instance of the application in that namespace.  Once the application has been provision, the tenant interacts with directly with the application.  

In each of these instances the following constructs are used to isolate tenants from each other: 

  + **Namespaces** are fundamental to implementing multi-tenancy.  They allow you to logical divide the cluster into logical partitions.  Moreover, quotas, network policies, service accounts, and other objects needed to implement multi-tenancy are scoped to a namespace.  
  + **Network policies** restrict communication between pods using labels or ip address ranges.  In a multi-tenant environment where strict network isolation between tenants is required, we recommend starting with a a deny rule that prevents applications from communicating across namespaces, another rule that allows all pods to query CoreDNS, and a rule that allows all communication within a namespace.  This can be further refined as required.  By default, all pods in all namespaces are allowed to communicate with each other. 
  + **RBAC** roles and bindings are the mechanism used to restrict the actions that can be performed againsts objects in different namespaces.  In the enterprise and KaaS use cases, RBAC can be used to delegate administration of namespaced objects to select groups of individuals.  
  + **Quotas** specify the maximum amount of CPU and memory and/or number of resoures, e.g. pods, for a namespace whereas **limit ranges** allow you to set default along with min/max for requets and limits.  The scheduler uses the aggregate requests for all the containers in a pod to determine which nodes it can schedule the pod onto.  Limits specify the maximum amount of CPU or memory that a container can consume from the node.  If the requests are set too low and the actual resource utilization exceeds the capacity of the node, the node will begin to experience CPU or memory pressure.  When this happens pods may be restarted and/or evicted from the node.  Overcommitting resources in a shared cluster is usually beneficial because it allows you maximize your resources.  Nevertheless, you should plan to impose quotas on namespaces in a multi-tenant environment as this will force tenant to specify requests and limits when scheduling their pods on the cluster.  You can also use quotas to apportion the cluster's resources to align with a tenant's spend.  
  + **Pod priority and pre-emption** can be useful when you want to provide different qualties of services for different customers.  For example, with pod priority you can configure pods from customer A to run at a higher priority than customer B. When there's insufficient capacity available, the Kubelet will kill the low priority pods from customer B to accommodate the high priority pods from customer A.  This can be especially handy in a SaaS environment.  

### Mitigating controls
Your chief concern as an administrator of a multi-tenant environment is preventing an attacker from gaining access to the underlying host. The following controls should be considered to mitigate the risk of this happening: 
  + **Pod Security Policies (PSPs)**: PSPs should be used to curtail the actions that can be performed by a container and to reduce a container's privileges, e.g. running as a non-root user.   
  + **Sandboxed execution environments for containers**: Not applicable to EKS, but useful in self-managed environments where you can configure alterate runtimes, e.g. [Kata Containers](https://github.com/kata-containers/documentation/wiki/Initial-release-of-Kata-Containers-with-Firecracker-support). Sandboxing is where you run a pod's containers within a micro VM like [Firecracker](https://firecracker-microvm.github.io/) as in Weave's [Firekube](https://www.weave.works/blog/firekube-fast-and-secure-kubernetes-clusters-using-weave-ignite). For additional information about the effort to make Firecracker a supported runtime for EKS, See https://threadreaderapp.com/thread/1238496944684597248.html. 
  + **Open Policy Agent (OPA) & [Gatekeeper](https://github.com/open-policy-agent/gatekeeper)**: Gatekeeper is a Kubernetes admission controller that enforces policies created with OPA. With OPA you can create a policy that runs pods from tenants on separate instances or at a higher priority than other tenants. 

### Hard multi-tenancy
Hard multi-tenancy can be implemented by provisioning separate clusters for each tenant.  While this provides very strong isolation between tenants, it has several drawbacks.  First, when you have lots of tenants, this approach can quickly become prohibitively expensive. Not only will you have to pay for the control plane costs for each cluster, you will not be able to share compute resources between clusters.  This will eventually cause fragmentation where subset of clusters are underutilized while others may be overutilized. Second, you will likely need to buy or build special tooling to manage all of these clusters.  In time, managing hundreds or thousands of clusters may simply become too unweildy.  Finally, creating a cluster per tenant will be slow relative to a creating a namespace. For a majority of cluster, the cluster per tenant model is not really practical or cost effective, yet it may be necessary in highly regulated industries or in SaaS environments where strong isolation is required. 

## Auditing and logging
Collecting and analyzing \[audit\] logs is useful for a variety of different reasons.  Logs can help with root cause analysis and attribution, i.e. ascribing a change to a particular user. When enough logs have been collected, they can be used to detect anomolous behaviors too. On EKS, the audit logs are sent to Amazon Cloudwatch Logs. The audit policy for EKS currently augments the reference [policy](https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh#L983-L1108) in the helper script with the following policy: 
```
- level: RequestResponse
    namespaces: ["kube-system"]
    verbs: ["update", "patch", "delete"]
    resources:
      - group: "" # core
        resources: ["configmaps"]
        resourceNames: ["aws-auth"]
    omitStages:
      - "RequestReceived"
```
This update logs changes to the `aws-auth` ConfigMap which is used to grant access to an EKS cluster. 

### Best Practices
+ Kubernetes audit logs include two annotations that indicate whether or not a request was authorized (authorization.k8s.io/decision) and the reason for the decision (authorization.k8s.io/reason).  Use these attributes to ascertain why a particular API call was allowed. 
   
+ Create an alarm to automatically alert you where there isn an increase in 403 Forbidden and 401 Unauthorized responses, and then use attributes like host, sourceIPs, and k8s_user.username to find out where those requests are coming from.

+ **Enable audit logs**. The audit logs are part of the EKS control plane logs.  Instructions for enabling/disabling the control plane logs, including the audit log, can be found here, https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html#enabling-control-plane-log-export. 

  > Be aware that the maximum size for a CWL entry is 256KB whereas the maximum Kubernetes API request size is 1.5MiB. The difference could allow an attacker to pad a API request with dummy data to obscure their tracks.
  
+ Use Cloudwatch Log Insights to monitor changes to RBAC objects, e.g. roles, rolebindings, clusterroles, and clusterrolebindings.  A few sample queries appear below: 

  Lists create, update, delete operations to roles
  ```
  fields @timestamp, @message
  | sort @timestamp desc
  | limit 100
  | filter objectRef.resource="roles" and verb in ["create", "update", "patch", "delete"]
  ```
  Lists create, update, delete operations to rolebindings
  ```
  fields @timestamp, @message
  | sort @timestamp desc
  | limit 100
  | filter objectRef.resource="rolebindings" and verb in ["create", "update", "patch", "delete"]
  ```
  Lists create, update, delete operations to clusterroles
  ```
  fields @timestamp, @message
  | sort @timestamp desc
  | limit 100
  | filter objectRef.resource="clusterroles" and verb in ["create", "update", "patch", "delete"]
  ```
  Lists create, update, delete operations to clusterrolebindings
  ```
  fields @timestamp, @message
  | sort @timestamp desc
  | limit 100
  | filter objectRef.resource="clusterrolebindings" and verb in ["create", "update", "patch", "delete"]
  ```
  Plots unauthorized read operations against secrets
  ```
  fields @timestamp, @message
  | sort @timestamp desc
  | limit 100
  | filter objectRef.resource="secrets" and verb in ["get", "watch", "list"] and responseStatus.code="401"
  | count() by bin(1m)
  ```
  List of failed anonymous requests
  ```
  fields @timestamp, @message, sourceIPs.0
  | sort @timestamp desc
  | limit 100
  | filter user.username="system:anonymous" and responseStatus.code in ["401", "403"]
  ```
### Additional resources
As the volume of logs increases, parsing and filtering them with Log Insights or another log analysis tool may become ineffective.  As an alternative, you might want to consider running [Sysdig Falco](https://github.com/falcosecurity/falco) and [ekscloudwatch](https://github.com/sysdiglabs/ekscloudwatch). Falco analyzes the logs and flags anomalies or abuse over an extended period of time. The ekscloudwatch project allows audit log events from EKS to be sent to Falco for analysis. Falco provides a set of [default audit rules](https://github.com/falcosecurity/falco/blob/master/rules/k8s_audit_rules.yaml) along with the ability to add your own. 

Yet another option might be to store the audit logs in S3 and use the SageMaker [Random Cut Forest](https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html) algorithm to anomalous behaviors that warrant further investigation.

An upcoming solution from Alcide called [kAudit](https://www.alcide.io/kaudit-K8s-forensics/) proclaims to identify suspicious activity patterns too. 

## Network security
Network security has a couple of different facets.  The first involves the application of rules that restrict the flow of network traffic between services.  The second involves the encryption of traffic while it is on the wire.  The mechanisms to implement these security measures on EKS are varied but often include the following items:
+ Network Policies
+ Security Groups
+ Encryption in transit
  + Service Mesh
  + Container Network Interfaces (CNIs)
  + Nitro Instances


### Network policy
Within a Kubernetes cluster, all pod to pod communication is allowed by default.  While this may help promote experimentation, it is not considered secure.  Kubernetes network policies give you a mechanism to restrict network traffic between pods or pods and external services.  The de facto policy engine for EKS is [Calico](https://docs.projectcalico.org/introduction/), an open source project from Tigera.  Network policies operate at layers 3 and 4 of the OSI model.  Rules can comprise of a src/dst address or port/protocol or a combination of both. Isovalent, the maintainers of [Cilium](https://cilium.readthedocs.io/en/stable/intro/), have extended the network policies to include partial support for layer 7 rules, e.g. HTTP.  Cilium also has support for DNS hostnames which can be useful for restricting traffic between Kubernetes services/pods and resources that run within or outside of your VPC. By contrast, Tigera Enterprise includes a feature that allows you to map a Kubernetes network policy to an AWS security group. 

  > When you first provision an EKS cluster, the Calico policy engine is not installed by default. The manifests for installing Calico can be found at https://github.com/aws/amazon-vpc-cni-k8s/tree/master/config.

Calico policies can be scoped to namespaces, pods, service accounts, or globally.  When policies are scoped to a service account, it associates a set of ingress/egress rules with that service account.  With the proper RBAC rules in place, you can prevent teams from overriding these rules, allowing IT security professionals to safely delegate administration of namespaces.

You can find a list of common Kubernetes network policies at https://github.com/ahmetb/kubernetes-network-policy-recipes.  A similar set of rules for Calico are available at https://docs.projectcalico.org/security/calico-network-policy. 

#### Recommendations
+ **Create a default deny policy**. As with RBAC policies, network policies should adhere to the policy of least privileged access.  Start by creating a deny all policy that restricts all inbound and outbound traffic from a namespace or create a global policy using Calico.

_Kubernetes network policy_
  ```
  apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: default-deny
    namespace: default
  spec:
    podSelector: {}
    policyTypes:
    - Ingress
    - Egress
  ```
  _Calico network policy_
  ```
  apiVersion: crd.projectcalico.org/v1
  kind: GlobalNetworkPolicy
  metadata:
    name: default-deny
  spec:
    selector: all()
    types:
    - Ingress
    - Egress
  ```

+ **Create a rule to allow DNS queries**. Once you have the defaul deny all rule in place, you can begin to layer on additional rules, such as a global rule that allows pods to query CoreDNS for name resolution. You begin by labeling the namespace: 
  ```
  kubectl label namespace kube-system name=kube-system
  ```
  Then add the network policy:
  ```
  apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-dns-access
    namespace: default
  spec:
    podSelector:
      matchLabels: {}
    policyTypes:
    - Egress
    egress:
    - to:
      - namespaceSelector:
          matchLabels:
            name: kube-system
      ports:
      - protocol: UDP
        port: 53
  ```
  _Calico global policy equivalent_
  ```
  apiVersion: crd.projectcalico.org/v1
  kind: GlobalNetworkPolicy
  metadata:
    name: allow-dns-egress
  spec:
    selector: all()
    types:
    - Egress
    egress:
    - action: Allow
      protocol: UDP  
      destination:
        namespaceSelector: name == "kube-system"
        ports: 
        - 53
  ```
  The following is an example of how to use associate a network policy to a service account while preventing users associated with the readonly-sa-group from editing the service account my-sa in the default namespace: 
  ```
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: my-sa
    namespace: default
    labels: 
      name: my-sa
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    namespace: default
    name: readonly-sa-role
  rules:
  # Allows the subject to read a service account called my-sa
  - apiGroups: [""]
    resources: ["serviceaccounts"]
    resourceNames: ["my-sa"]
    verbs: ["get", "watch", "list"]
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    namespace: default
    name: readonly-sa-rolebinding
  # Binds the readonly-sa-role to the RBAC group called readonly-sa-group
  subjects:
  - kind: Group 
    name: readonly-sa-group 
    apiGroup: rbac.authorization.k8s.io 
  roleRef:
    kind: Role 
    name: readonly-sa-role 
    apiGroup: rbac.authorization.k8s.io
  ---
  apiVersion: crd.projectcalico.org/v1
  kind: NetworkPolicy
  metadata:
    name: netpol-sa-demo
    namespace: default
  # Allows all ingress traffic to services in the default namespace that reference
  # the service account called my-sa
  spec:
    ingress:
      - action: Allow
        source:
          serviceAccounts:
            selector: 'name == "my-sa"'
    selector: all()
  ```
+ **Incrementally add rules to selectively allow the flow of traffic between namespaces/pods**.  Start by allowing pods within a namespace to communicate with each other and then add custom rules that further restrict pod to pod communication within that namespace. 
+ **Log network traffic metadata**. VPC Flow Logs captures metadata about the traffic flowing through a VPC, such as source and destination IP address and port along with accepted/dropped packets. This information could be analyzed to look for suspicous or unusual activity between resources within the VPC, including pods.  However, since the IP addresses of pods frequently change as they replaced, Flow Logs may not be sufficient on its own.  Calico Enterprise extends the Flow Logs with pod labels, making it easier to decipher the traffic flows between pods.  It also makes use of machine learning to identify anomalous traffic.
+ **Use encryption with AWS load balancers**. The ALB and NLB both have support for transport encryption (SSL and TLS).  The `alb.ingress.kubernetes.io/certificate-arn` annotation for the ALB lets you to specify which certificates to add to the ALB.  If you omit the annotation the controller will attempt to add certificates to listeners that require it by matching the available ACM certiciates using the host field. Starting with EKS v1.15 you can use the service.beta.kubernetes.io/aws-load-balancer-ssl-cert annotation with the NLB as shown in the example below. 

```
apiVersion: v1
kind: Service
metadata:
  name: demo-app
  namespace: default
  labels:
    app: demo-app
  annotations:
     service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
     service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "<certificate ARN>"
     service.beta.kubernetes.io/aws-load-balancer-ssl-ports: "443"
     service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
spec:
  type: LoadBalancer
  ports:
  - port: 443
    targetPort: 80
    protocol: TCP
  selector:
    app: demo-app
---
kind: Deployment
apiVersion: apps/v1
metadata:
  name: nginx
  namespace: default
  labels:
    app: demo-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo-app
  template:
    metadata:
      labels:
        app: demo-app
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 443
              protocol: TCP
            - containerPort: 80
              protocol: TCP
```

#### Additional Resources
+ [Kubernetes & Tigera: Network Policies, Security, and Audit](https://youtu.be/lEY2WnRHYpg) 
+ [Cilium](https://cilium.readthedocs.io/en/stable/intro/)
+ [Calico Enterprise](https://www.tigera.io/tigera-products/calico-enterprise/)

### Security groups
EKS uses security groups (SGs) to control the traffic between the EKS control plane and the cluster's worker nodes. Security groups are also used to control the traffic between worker nodes, worker nodes and other VPC resources, and external IP addresses.  When you provision a 1.14 cluster at platform version eks.3 or greater a cluster security group is automatically created for you.  The security group allows unfettered communication between the EKS control plane and the nodes from managed node groups. For simplicity, it is recommended that you add the cluster SG to all node groups, including unmanaged node groups.

Prior to 1.14 platform version eks.3 there were separate security groups configured for the EKS control plane and node groups. The minimum and suggested rules for the control plan and node group security groups can be found at https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html.  The minimum rules for the control plane security group allows port 443 inbound from the worker node SG and is there for securely communicating with the Kubernetes API server.  It also allows port 10250 outbound to the worker node SG; the port that the kubelet listens on. The minimum node group rules allow port 10250 inbound from the control plane SG and 443 outbound to the control plane SG.  Finally there is a rule that allows unfettered communication between nodes within a node group. 

If you need to control communication between services that run within the cluster and service the run outside the cluster, consider using a network policy engine like Cilium which allows you to use a DNS name.  Alternatively, use Calico Enterprise which allows you to map a network policy to an AWS security group.  If you're implemting a service mesh like Istio, you can use an egress gateway to restrict network egress to specific fully qualified domains or IP addresses. Read the 3 part series on [egress traffic control in Istio](https://istio.io/blog/2019/egress-traffic-control-in-istio-part-1/) for further information. 

> Unless you are running 1 pod per instance or dedicating a set of instances to run a particular application, security groups are considered to be too course grained to control network traffic.  Contemplate using network policies which are Kubernetes-aware instead. 

### Encryption in transit
Applications that need to conform to PCI, HIPAA, or other regulations may need to encrypt data while it is in transit.  Nowadays TLS is the de facto choice for encrypting traffic on the wire.  TLS, like it's predecessor SSL, provides secure communications over a network using cyptographic protocols.  TLS uses symmetric encryption where the keys to encrypt the data are generated based on a shared secret that is negotiated at the beginning of the sesssion. The follow are a few ways that you can encrypt data in a Kubernetes environment. 
+ **Nitro Instances**. Traffic exchanged between select Nitro instance types, e.g. M5n, M5dn, R5n, and R5dn, is automatically encrypted by default.  When there's an intermediate hop, like a trasit gateway or a load balancer, the traffic is not encrypted. 
+ **Container Network Interfaces (CNIs)**. [WeaveNet](https://www.weave.works/oss/net/) can be configured to automatically encrypt all traffic using NaCl encryption for sleeve traffic, and IPsec ESP for fast datapath traffic.
+ **Service Mesh**. Encryption in transit can also be implemented with a service mesh like App Mesh, Linkerd v2, and Istio. Currently, App Mesh supports [TLS encryption](https://docs.aws.amazon.com/app-mesh/latest/userguide/virtual-node-tls.html) with a private certificate issued by ACM or a certificate stored on the local file system of the virtual node. Linkerd and Istio both have support for mTLS which adds another layer of security through mutual exchange and validation of certificates.

## Encryption at rest
There are 3 different AWS-native storage options you can use with Kubernetes: EBS, EFS, and FSx for Lustre.  All 3 offer encryption at rest using a service managed key or a customer master key (CMK). For EBS you can use the in-tree storage driver or the [EBS CSI driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver).  Both include parameters for encrypting volumes and supplying a CMK.  For EFS, you can use the [EFS CSI driver](https://github.com/kubernetes-sigs/aws-efs-csi-driver), however, unlike EBS, the EFS CSI driver does not support dynamic provisioning.  If you want to use EFS with EKS, you will need to provision and configure at-rest encryption for the file system prior to creating a PV. For further information about EFS file encryption, please refer to [Encrypting Data at Rest](https://docs.aws.amazon.com/efs/latest/ug/encryption-at-rest.html). Besides offering at-rest encryption, EFS and FSx for Lustre include an option for encrypting data in transit.  FSx for Luster does this by default.  For EFS, you can add transport encryption by adding the `tls` parameter to `mountOptions` in your PV as in this example: 
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: efs-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: efs-sc
  mountOptions:
    - tls
  csi:
    driver: efs.csi.aws.com
    volumeHandle: <file_system_id>
```
The [FSx CSI driver](https://github.com/kubernetes-sigs/aws-fsx-csi-driver) supports dynamic provisioning of Lustre file systems.  It encrypts data with a service managed key by default, although there is an option to provide you own CMK as in this example:
```
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fsx-sc
provisioner: fsx.csi.aws.com
parameters:
  subnetId: subnet-056da83524edbe641
  securityGroupIds: sg-086f61ea73388fb6b
  deploymentType: PERSISTENT_1
  kmsKeyId: <kms_arn>
``` 
### Recommendations
+ **Encrypt data at rest**.  Encrypting data at rest is considered a best practice.  If you're unsure whether encryption is necessary, encrypt your data. 
+ **Rotate your CMKs periodically**. Configure KMS to automatically rotate you CMKs.  This will rotate your keys once a year while saving old keys indefinitely so that your data can still be decrypted.  For additional information see [Rotating customer master keys](https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html)
+ **Use EFS access points to simplify access to shared datasets**. If you have shared datasets with different POSIX file permissions or want to restrict access to part of the shared file system by creating different mount points, consider using EFS access points. To learn more about working with access points, see https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html. Today, if you want to use access point (AP) you'll need to reference the AP in the PV's `volumeHandle` parameter.

## Runtime security 
Runtime security provides active protection for your containers while they're running.  The idea is to detect and/or prevent malicious activity from occuring inside the container. With secure computing (seccomp) you can prevent a containerized application from making certain syscalls to the underlying host operating system's kernel. While the Linux operating system has a few hundred system calls, the lion's share of them are not necessary for running containers. By restricting what syscalls can be made by a container, you can effectively decrease your application's attack surface. To get started with seccomp, analyze the results of a stack trace to see which calls your application is making or use a tool such as [syscall2seccomp](https://github.com/antitree/syscall2seccomp).

> Seccomp profiles are a Kubelet alpha feature.  You'll need to add the `--seccomp-profile-root` flag to the Kubelet arguments to make use of this feature. 

AppArmor is similar to seccomp, only it restricts an container's capabilities including accessing parts of the file system. It can be run in either enforcement or complain mode. Since building Apparmor profiles can be challenging, it is recommended you use a tool like [bane](https://github.com/genuinetools/bane) instead. 

> Apparmor is only available Ubuntu/Debian distributions of Linux. 

> Kubernetes does not currently provide any native mechanisms for loading AppArmor or seccomp profiles onto nodes.  They either have to be loaded manually or installed onto nodes when they are bootstrapped.  This has to be done prior to referencing them in your Pods because the scheduler is unaware of which nodes have profiles. 

### Recommendations
+ **Use a 3rd party solution for runtime defense**. Creating and managing seccomp and Apparmor profiles can be difficult if you're not familiar with Linux security.  If you don't have the time to become proficient, consider using a commercial solution.  A lot of them have moved beyond static profiles like Apparmor and seccomp and have begun using machine learning to block or alert on suspicious activity. 
+ **Consider add/dropping Linux capabilities before writing seccomp policies**. Capabilities involve various checks in kernel functions reachable by syscalls. If the check fails, the syscall typically returns an error. The check can be done either right at the beginning of a specific syscall, or deeper in the kernel in areas that might be reachable through multiple different syscalls (such as writing to a specific privileged file).  Seccomp, on the other hand, is a syscall filter which is applied to all syscalls before they are run. A process can set up a filter which allows them to revoke their right to run certain syscalls, or specific arguments for certain syscalls. 

Before using seccomp, consider whether adding/removing Linux capabilities gives you the control you need. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container for further information. 
+ **See whether you can accomplish your aims by using Pod Security Policies (PSPs)**. Pod Security Policies offer a lot of different ways to improve your security posture without introducing undue complexity.  Explore the options available in PSPs before venturing into building seccomp and Apparmor profiles. 

### Additional Resources
+ https://itnext.io/seccomp-in-kubernetes-part-i-7-things-you-should-know-before-you-even-start-97502ad6b6d6
+ https://github.com/kubernetes/kubernetes/tree/master/test/images/apparmor-loader
+ https://kubernetes.io/docs/tutorials/clusters/apparmor/#setting-up-nodes-with-profiles

### Tools
+ [Aqua](https://www.aquasec.com/products/aqua-cloud-native-security-platform/)
+ [Qualys](https://www.qualys.com/apps/container-security/)
+ [Sysdig Secure](https://sysdig.com/products/kubernetes-security/)
+ [Twistlock](https://www.twistlock.com/platform/runtime-defense/)

## Secrets management
Kubernetes secrets are used to store sensitive information, such as user certificates, passwords, or API keys. They are persisted in etcd as base64 encoded strings.  On EKS, the EBS volumes for etcd nodes are encypted with [EBS encryption](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html).  A pod can retrieve a Kubernetes secrets objects by referencing the secret in the podSpec.  These secrets can either be mapped to an environment variable or mounted as volume. For additional information on creating secrets, see https://kubernetes.io/docs/concepts/configuration/secret/. 

> Note: Secrets in a particular namespace can be referenced by all pods in the secret's namespace.

> Note: The node authorizer allows the Kubelet to read all of the secrets mounted to the node. 

### Recommendations
+ **Use separate namespaces as a way to isolate secrets from different applications**. If you have secrets that cannot be shared between applications in a namespace, create a separate namespace for those applications.  
+ **Use volume mounts instead of environment variables**. The values of environment variables can unintentionally appear in logs. Secrets mounted as volumes are instatiated as tmpfs volumes (a RAM backed file system) that are automatically removed from the node when the pod is deleted. 
+ **Use an external secrets provider**. There are several viable alternatives to using Kubernetes secrets, include Bitnami's [Sealed Secrets](https://github.com/bitnami-labs/sealed-secrets) and Hashicorp's [Vault](
https://www.hashicorp.com/blog/injecting-vault-secrets-into-kubernetes-pods-via-a-sidecar/). Unlike Kubernetes secrets which can be shared amongst all of the pods within a namespace, Vault gives you the ability to limit access to particular pods through the use of Kubernetes service accounts.  It also has support for secret rotation.  If Vault is not to your liking, you can use similar approach with AWS Secrets Manager, as in this example https://github.com/jicowan/secret-sidecar or you could try using a [serverless](https://github.com/mhausenblas/nase) mutating webhook instead.
+ **Audit the use of secrets**. On EKS, turn on audit logging and create a CloudWatch metrics filter and alarm to alert you when a secret is used (optional). The following is an example of a metrics filter for the Kubernetes audit log, `{($.verb="get") && ($.objectRef.resource="secret")}`.  You can also use the following queries with CloudWatchLog Insights: 
    ```
    fields @timestamp, @message
    | sort @timestamp desc
    | limit 100
    | stats count(*) by objectRef.name as secret
    | filter verb="get" and objectRef.resource="secrets"
    ```
    The above query will display the number of times a secret has been accessed within a specific timeframe. 
    ```
    fields @timestamp, @message
    | sort @timestamp desc
    | limit 100
    | filter verb="get" and objectRef.resource="secrets"
    | display objectRef.namespace, objectRef.name, user.username, responseStatus.code
    ```
    This query will display the secret, along with the namespace and username of the user who attempted to access the secret and the response code. 
+ **Rotate your secrets periodically**. Kubernetes doesn't automatically rotate secrets.  If you have to rotate secrets, consider using an external secret store, e.g. Vault or AWS Secrets Manager. 
+ **Use AWS KMS for envelop encryption of Kubernetes secrets** ([coming soon](https://github.com/aws/containers-roadmap/issues/530)). When this option becomes available, Kubernetes will encrypt your secrets with a unique data encryption key (DEK). The DEK is then encypted using a key encryption key (KEK) from AWS KMS which can be automatically rotated on a recurring schedule. With the KMS plugin for Kubernetes, all Kubernetes secrets are stored in etcd in ciphertext instead of plain text and can only be decrypted by the Kubernetes API server. 
  + https://aws.amazon.com/blogs/containers/using-eks-encryption-provider-support-for-defense-in-depth/

## Protecting the infrastructure (hosts)
+ **Run SELinux**. SELinux provides an additional layer of security to keep containers isolated from each other and from the host. SELinux allows administrators to enforce mandatory access controls (MAC) for every user, application, process, and file.
Use a OS optimized for running containers, e.g. Flatcar Linux, Project Atomic, RancherOS, and [Bottlerocket](https://github.com/bottlerocket-os/bottlerocket/), a new OS from AWS that has a read only root file system and uses partition flips for fast and reliable system updates.  A majority of these operating systems, like Bottlerocket, have been substantially paired down and optimized to run containers. 
  **Additional resources**
  + https://platform9.com/blog/selinux-kubernetes-rbac-and-shipping-security-policies-for-on-prem-applications/
  + https://jayunit100.blogspot.com/2019/07/iterative-hardening-of-kubernetes-and.html 
+ **Treat your infrastructure as immutable**.  Rather than performing in-place upgrades, replace your workers when a new patch or update becomes available. This can be approached a couple of ways. You can either add instances to an existing autoscaling group using the latest AMI as you sequentially cordon and drain nodes until all of the nodes in the group have been replaced with the latest AMI.  Alternatively, you can add instances to a new node group while you sequentally cordon and drain nodes from the old node group until all of the nodes have been replaced.  EKS [managed node groups](https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html) utilizes the second approach and will present an option to upgrade workers when a new AMI becomes available. `eksctl` also has a mechanism for creating node groups with the latest AMI and for gracefully cordoning and draining pods from nodes groups before the instances are terminated. If you decide to use a different method for replacing your worker nodes, it is recommended that you automate the process to minimize human oversight as you will likely need to replace workers regularly as new updates/patches are released and when the control plane is upgraded. 
With EKS Fargate, AWS will automatically update the underlying infrastructure as updates become available.  Oftentimes this can be done seamlessly, but there may be times when an update will cause your task to be rescheduled.  Hence, we recommend that you create deployments with multiple replicas when running your application as a Fargate pod. 
+ **Periodically run [kube-bench](https://github.com/aquasecurity/kube-bench) to verify compliance with [CIS benchmarks for Kubernetes](https://www.cisecurity.org/benchmark/kubernetes/)**. When running kube-bench against an EKS cluster, follow these instructions, https://github.com/aquasecurity/kube-bench#running-in-an-eks-cluster. Be aware that false positives may appear in the report because of the way the he EKS optimized AMI configures the kubelet.  See https://github.com/aquasecurity/kube-bench/issues/571 for further information. 
+ **Minimize access to worker nodes**. Instead of enabling SSH access, use [SSM Session Manager](https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html) when you need to remote into a host.  Unlike SSH keys which can be lost, copied, or shared, Session Manager allows you to control access to EC2 instances using IAM.  Moreover, it provides an audit trail and log of the commands that were run on the instance.  If you've lost access to the host through SSH or Session Manager, you can try running a node-shell, a privileged pod that gives you shell access on the node. An example appears below: 
  ```
  kind: Pod
  apiVersion: v1
  metadata:
    name: node-shell
    namespace: kube-system
    annotations:
      kubernetes.io/psp: eks.privileged
  spec:
    containers:
      - name: shell
        image: 'docker.io/alpine:3.9'
        command:
          - nsenter
        args:
          - '-t'
          - '1'
          - '-m'
          - '-u'
          - '-i'
          - '-n'
          - sleep
          - '14000'
        securityContext:
          privileged: true
    restartPolicy: Never
    nodeSelector:
      kubernetes.io/hostname: ip-192-168-49-62.us-west-2.compute.internal
    nodeName: ip-192-168-49-62.us-west-2.compute.internal
    hostNetwork: true
    hostPID: true
    hostIPC: true
    enableServiceLinks: true
    ```
+ **Deploy workers onto private subnets**. By deploying workers onto private subnets, you minimize their exposure to the Internet where attacks often originate.  At present, worker nodes that are part of a managed node group are are automatically assigned a public IP. If you plan to use managed node groups use AWS security groups to restrict or deny inbound access from the Internet (0.0.0.0/0). Risk to workers that are deployed onto public subnets can also be mitigated by implementing restrictive security group rules. 

+ **Run [Amazon Inspector](https://docs.aws.amazon.com/inspector/latest/userguide/inspector_introduction.html) to assesses applications for exposure, vulnerabilities, and deviations from best practices**.  It requires the deployment of an agent that continually monitors activity on the instance while using set of rules to assess alignment with best practices. 
  At present, managed node groups do not allow you to supply user metadata or your own AMI.  If you want to run Inspector on managed workers, you will need to install the agent after the node has been bootstrapped.  
  Inspector cannot be run on the infrastructure used to run Fargate pods. 
  **Alternatives**
  + [Sysdig Secure](https://sysdig.com/products/kubernetes-security/)

### Tools
+ [Keiko](https://github.com/keikoproj/keiko)

## Compliance
Compliance is a shared responsibility between AWS and the consumers of its services. Generally speaking, AWS is responsible for “security of the cloud” whereas its users are responsible for “security in the cloud.” The line that delineates what AWS and its users are responsible for will vary depending on the service. For example, with Fargate, AWS is responsible for managing the physical security of its data centers, the hardware, the virtual infrastructure (Amazon EC2), and the container runtime (Docker). Users of Fargate are responsible for securing the container image and their application. Knowing who is responsible for what is an important consideration when running workloads that must adhere to compliance standards. 
The following table shows the compliance programs with which the different container services conform.

| Compliance Program | Amazon ECS | Amazon EKS | AWS Fargate | Amazon ECR |
| ------------------ |:----------:|:----------:|:-----------:|:----------:|
| PCI DSS Level 1	| 1 |	1 |	1 |	1 |
| HIPAA Eligible	| 1 |	1	| 1	| 1 |
| SOC I |	1 |	1 |	1 |	1 |
| SOC II | 1 |	1 |	1 |	1 |
| SOC III |	1 |	1 |	1 |	1 |
| ISO 27001 |	1 |	1 |	1 |	1 |
| ISO 9001 | 1 |	1 |	1 |	1 |
| ISO 27017 |	1 |	1 |	1 |	1 |
| ISO 27018 |	1 |	1 |	1 |	1 |
| IRAP | 1 | 0 | 0 | 0 |
| FedRAMP | | | | |
| JAB Review | 0 | 0 | JAB Review | |
| DOD CC SRG | JAB Review |	0 |	0 |	JAB Review |
| MTCS | 1 | 0 | 0 | 1 |
| C5 | 1 | 0 | 0 | 1 |
| K-ISMS | 0 | 0 | 0 | 0 |
| ENS High | 1 | 0 | 1 | 0 |

### Tools
+ kube-bench
+ docker-bench-security
+ actuary

## Incident response and forensics
Your ability to react quickly to an incident can help minimize damage caused from a breach. Having a reliable alerting system that can warn you of suspicious behavior is the first step in a good incident response plan. When an incident does arise you have to quickly decide whether to destroy and replace the container, or isolate and inspect the container. There are different factors that may lead to the either decision, but if you choose to isolate the worker node for forensic investigation and root cause analysis, then the following set of activities should be followed:

+ Identify the offending pod and worker node.
+ Enable termination protection on impacted worker node.
+ Revoke temporary security credentials assigned to the pod or worker node if necessary.
+ Cordon the worker node.
+ Isolate the pod by creating a network policy the denies all ingress and egress traffic
+ Label offending the pod/node with a label indicating that it is part of an active investigation.
+ Capture volatile artifacts in runtime on the worker node.
  + Memory capture operating system memory. This will capture the docker daemon and its subprocess per  container.
  + Perform a netstat tree dump of the processes running and the open ports. This will capture the docker daemon and its subprocess per container. 
+ Run docker commands before evidence is altered on the worker node.
    + Docker container top CONTAINER for processes running.
    + Docker container logs CONTAINER for daemon level held logs.
    + Docker container port CONTAINER for list of open ports.
    + Docker container diff CONTAINER to capture changes to files and directories to container's  filesystem since its initial launch.   
+ Pause the container for forensic capture.
+ Snapshot the container instance EBS volume.
+ Isolate the worker node from the network by remove it from its node security groups.

### Recommendations
+ **Practice security game days**. Divide your security practitioners into 2 teams: red and blue.  The red team will be focused on probing different systems for vulnerabilities while the blue team will be responsible for defending against them.  If you don't have enough security practitioners to create separate teams, consider hiring an outside entity that has knowledge of Kubernetes exploits. 
+ **Run penetration tests against your cluster**. Periodically attacking your own cluster can help you discover vulnerabilities and misconfigurations.  Before getting started, follow the [penetration test guidelines](https://aws.amazon.com/security/penetration-testing/) before conducting a test against your cluster. 

### Tools
+ [kube-hunter](https://github.com/aquasecurity/kube-hunter)
+ [Gremlin](https://www.gremlin.com/product/#kubernetes)
+ https://twitter.com/IanColdwater