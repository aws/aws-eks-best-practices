---
日期: 2023-09-22
作者:
  - Shane Corbett
---
# 節點和工作負載效率
提高工作負載和節點的效率可以降低複雜性/成本,同時提高性能和擴展能力。在規劃此效率時,需要考慮許多因素,從各種權衡取捨的角度思考比遵循每個功能的最佳做法設置更容易。讓我們在下一節深入探討這些權衡。

## 節點選擇
使用稍大的節點大小(4-12xlarge)可以增加我們運行 pod 的可用空間,因為它減少了用於"開銷"的節點百分比,例如 [DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) 和為系統組件保留的 [Reserves](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/)。在下圖中,我們可以看到僅有適度數量的 DaemonSets 時,2xlarge 與 8xlarge 系統的可用空間之間的差異。

!!! 注意
    由於 k8s 通常是水平擴展,因此對於大多數應用程序,採用 NUMA 大小節點的性能影響是沒有意義的,因此下面的建議範圍低於該節點大小。

![節點大小](../images/node-size.png)

較大的節點大小允許我們擁有更高百分比的每個節點可用空間。但是,如果將節點填滿太多 pod 導致錯誤或使節點飽和,這種模式就會走極端。監控節點飽和度對於成功使用較大的節點大小至關重要。

節點選擇很少是一刀切的解決方案。通常最好將具有顯著不同更新率的工作負載分割到不同的節點組。具有高更新率的小批量工作負載最適合 4xlarge 系列實例,而像 Kafka 這樣大規模應用程序需要 8 vCPU 且更新率低,則更適合 12xlarge 系列。

![更新率](../images/churn-rate.png)

!!! 提示
    使用非常大的節點大小時需要考慮的另一個因素是,由於 CGROUPS 沒有隱藏容器化應用程序的總 vCPU 數量,動態運行時可能會產生意外數量的 OS 線程,從而導致難以排查的延遲。對於這些應用程序,建議使用 [CPU 固定](https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy)。要深入探討此主題,請參閱以下視頻 https://www.youtube.com/watch?v=NqtfDy_KAqg

## 節點裝箱
### Kubernetes 與 Linux 規則
在處理 Kubernetes 上的工作負載時,我們需要注意兩組規則。Kubernetes 調度程序使用請求值在節點上調度 pod 的規則,以及 pod 調度後發生的情況,這屬於 Linux 而非 Kubernetes 的範疇。

Kubernetes 調度程序完成後,一組新的規則開始生效,即 Linux 完全公平調度程序 (CFS)。關鍵要點是 Linux CFS 沒有核心的概念。我們將討論為什麼以核心思考會導致優化工作負載擴展性的重大問題。

### 以核心思考
混淆的起源在於 Kubernetes 調度程序確實有核心的概念。從 Kubernetes 調度程序的角度來看,如果我們查看一個節點上有 4 個 NGINX pod,每個都請求一個核心,該節點看起來如下所示。

![](../images/cores-1.png)

但是,讓我們思考一下從 Linux CFS 的角度來看這種情況會是怎樣的。使用 Linux CFS 系統時最重要的是要記住:僅有忙碌的容器 (CGROUPS) 才會計入共享系統。在這種情況下,只有第一個容器很忙,因此它可以使用節點上的所有 4 個核心。

![](../images/cores-2.png)

為什麼這很重要?假設我們在開發集群中進行了性能測試,在該集群中 NGINX 應用程序是該節點上唯一忙碌的容器。當我們將應用程序移至生產環境時,會發生以下情況:NGINX 應用程序需要 4 個 vCPU 的資源,但由於節點上的所有其他 pod 都很忙,因此我們的應用程序性能會受到限制。

![](../images/cores-3.png)

這種情況會導致我們不必要地添加更多容器,因為我們沒有允許應用程序擴展到其"最佳點"。讓我們更詳細地探討一下這個重要的"最佳點"概念。

### 應用程序適當調整大小
每個應用程序都有一個無法再處理更多流量的點。超過這個點會增加處理時間,甚至在遠遠超過這個點時會丟棄流量。這就是應用程序的飽和點。為了避免出現擴展問題,我們應該嘗試在應用程序達到其飽和點之前對其進行擴展。讓我們稱這一點為最佳點。

![最佳點](../images/sweet-spot.png)

我們需要測試每個應用程序以了解其最佳點。在這裡不會有通用指導,因為每個應用程序都不同。在此測試過程中,我們試圖了解最能顯示應用程序飽和點的最佳指標。通常使用利用率指標來表示應用程序已飽和,但這很快會導致擴展問題(我們將在後面的部分詳細探討這個主題)。一旦我們找到這個"最佳點",就可以使用它來有效地擴展我們的工作負載。

相反,如果我們在最佳點之前就擴展並創建了不必要的 pod,會發生什麼情況?讓我們在下一節探討這一點。

### Pod 蔓延
為了了解創建不必要的 pod 可能會迅速失控,讓我們看左側的第一個示例。當處理每秒 100 個請求時,此容器的正確垂直規模約佔 2 個 vCPU 的利用率。但是,如果我們將請求值設置為半個核心而低估了,我們現在需要為每個實際需要的 pod 創建 4 個 pod。如果我們的 [HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) 設置為默認的 50% CPU,那麼這個問題會進一步加劇,因為那些 pod 會以一半的空閒狀態進行擴展,從而創建 8:1 的比率。

![](../images/scaling-ratio.png)

將這個問題擴大,我們很快就會看到它是如何失控的。如果十個 pod 的部署的最佳點設置不正確,很快就會激增到 80 個 pod 以及運行它們所需的額外基礎設施。

![](../images/bad-sweetspot.png)

現在我們已經了解到不允許應用程序在其最佳點運行的影響,讓我們回到節點級別,問一下為什麼 Kubernetes 調度程序和 Linux CFS 之間的這種差異如此重要?

在使用 HPA 進行擴展時,我們可能會遇到一種情況,即我們有很多空間可以分配更多 pod。但這是一個錯誤的決定,因為左側所示的節點已經達到 100% CPU 利用率。在一個不切實際但理論上可能發生的極端情況下,我們可能會遇到另一種極端情況,即我們的節點完全滿載,但 CPU 利用率為零。

![](../images/hpa-utilization.png)
### 設置請求
很容易想將請求值設置為該應用程序的"最佳點"值,但這會導致效率低下,如下圖所示。在這裡,我們將請求值設置為 2 vCPU,但這些 pod 的平均利用率大多時候只有 1 個 CPU。這種設置會導致我們浪費 50% 的 CPU 週期,這是不可接受的。

![](../images/requests-1.png)

這帶來了一個複雜的答案。容器利用率不能被孤立地考慮;必須將節點上運行的其他應用程序納入考慮範圍。在下面的示例中,突發性 CPU 利用率較高的容器與兩個低 CPU 利用率的容器混合在一起,後者可能受到內存限制。通過這種方式,我們允許容器達到其最佳點,而不會使節點飽和。

![](../images/requests-2.png)

需要牢記的重要概念是,使用 Kubernetes 調度程序的核心概念來理解 Linux 容器性能會導致決策失誤,因為它們無關。

!!! 提示
    Linux CFS 有其強大之處。這對於 I/O 密集型工作負載尤其如此。但是,如果您的應用程序使用完整的核心而沒有側車,並且沒有 I/O 要求,則 CPU 固定可以消除此過程中的大量複雡性,並在有這些警告的情況下建議使用。

## 利用率與飽和度
應用程序擴展中的一個常見錯誤是僅使用 CPU 利用率作為擴展指標。對於複雜的應用程序,這幾乎總是一個糟糕的指標,無法表示應用程序是否真正飽和了請求。在左側的示例中,我們看到所有請求實際上都會命中 Web 服務器,因此 CPU 利用率與飽和度是一致的。

在真實世界的應用程序中,很可能有一些請求將由數據庫層或身份驗證層等服務。在這種更常見的情況下,請注意 CPU 與飽和度不一致,因為請求正在由其他實體服務。在這種情況下,CPU 是一個非常糟糕的飽和指標。

![](../images/util-vs-saturation-1.png)

在應用程序性能中使用錯誤的指標是 Kubernetes 中不必要和不可預測的擴展的首要原因。在選擇正確的飽和指標方面必須格外小心。重要的是要注意,無法給出一種適用於所有情況的建議。根據所使用的語言和應用程序類型的不同,飽和度指標也是多種多樣的。

我們可能認為這個問題只存在於 CPU 利用率中,但其他常見的指標,如每秒請求數,也可能遇到完全相同的問題。請注意,請求也可能會進入 DB 層、身份驗證層,而不是直接由我們的 Web 服務器服務,因此它是 Web 服務器本身真正飽和度的一個糟糕指標。

![](../images/util-vs-saturation-2.png)

不幸的是,在選擇正確的飽和指標時沒有簡單的答案。以下是一些需要考慮的指導原則:

* 了解您的語言運行時 - 具有多個 OS 線程的語言與單線程應用程序的反應會有所不同,因此對節點的影響也不同。
* 了解正確的垂直擴展規模 - 在擴展新 pod 之前,您希望在應用程序的垂直擴展中保留多少緩衝區?
* 哪些指標真正反映了您應用程序的飽和度 - Kafka 生產者的飽和度指標與複雜 Web 應用程序的指標會有很大不同。
* 節點上的所有其他應用程序對彼此有何影響 - 應用程序性能不是在真空中進行的,節點上的其他工作負載會產生重大影響。

結束本節時,很容易認為上述內容過於複雜和不必要。通常情況下,我們會遇到問題,但由於查看的是錯誤的指標,因此我們無法了解問題的真正本質。在下一節中,我們將看到這種情況是如何發生的。

### 節點飽和度
現在我們已經探討了應用程序飽和度,讓我們從節點的角度來看這個概念。讓我們看看 100% 利用的兩個 vCPU 之間利用率與飽和度的區別。

左側的 vCPU 100% 利用,但沒有其他任務等待在該 vCPU 上運行,因此從純理論的角度來看,這是非常高效的。同時,我們有 20 個單線程應用程序等待由一個 vCPU 處理。現在所有 20 個應用程序都會在等待輪到由 vCPU 處理時遇到某種延遲。換句話說,右側的 vCPU 已飽和。

如果我們只查看利用率,我們不僅無法看到這個問題,而且可能會將這種延遲歸咎於與網絡相關的其他原因,從而導致我們走上了錯誤的道路。

![](../images/node-saturation.png)

在任何給定時間增加節點上運行的 pod 總數時,查看飽和度指標而不僅僅是利用率指標非常重要,因為我們很容易忽視了節點過度飽和的事實。對於此任務,我們可以使用壓力停滯信息指標,如下圖所示。

PromQL - 停滯的 I/O

```
topk(3, ((irate(node_pressure_io_stalled_seconds_total[1m])) * 100))
```

![](../images/stalled-io.png)

!!! 注意
    有關壓力停滯指標的更多信息,請參閱 https://facebookmicrosites.github.io/psi/docs/overview*

使用這些指標,我們可以看到應用程序中所有線程正在使用多少 vCPU。為此,我們將使用指標 `container_cpu_usage_seconds_total`。

由於節流邏輯每 100 毫秒發生一次,而這個指標是每秒指標,我們將使用 PromQL 來匹配這個 100 毫秒的週期。如果您想深入了解這個 PromQL 語句的工作原理,請參閱以下 [博客](https://aws.amazon.com/blogs/containers/using-prometheus-to-avoid-disasters-with-kubernetes-cpu-limits/)。

PromQL 查詢:

```
topk(3, max by (pod, container)(rate(container_cpu_usage_seconds_total{image!="", instance="$instance"}[$__rate_interval]))) / 10
```

![](../images/cpu-1.png)

一旦我們確信找到了正確的值,就可以將限制投入生產。然後,有必要查看我們的應用程序是否由於某些意外原因而受到節流。我們可以通過查看 `container_cpu_throttled_seconds_total` 來做到這一點。

```
topk(3, max by (pod, container)(rate(container_cpu_cfs_throttled_seconds_total{image!=``""``, instance=``"$instance"``}[$__rate_interval]))) / 10
```

![](../images/cpu-2.png)
### 內存
內存分配是另一個很容易將 Kubernetes 調度行為與 Linux CGroup 行為混淆的示例。這是一個更微妙的主題,因為 Linux 中的 CGroup v2 在處理內存方面發生了重大變化,Kubernetes 也相應地更改了其語法;請閱讀這篇 [博客](https://kubernetes.io/blog/2021/11/26/qos-memory-resources/) 以了解更多詳情。

與 CPU 請求不同,內存請求在調度過程完成後就不再使用。這是因為我們無法像 CPU 一樣在 CGroup v1 中壓縮內存。這使我們只剩下內存限制,它旨在通過終止 pod 來防止內存洩漏,這是一種全有或全無的方式,但現在我們有了新的方式來解決這個問題。

首先,重要的是要理解為容器設置正確的內存量並不像看起來那麼簡單。Linux 中的文件系統會將內存用作緩存以提高性能。這個緩存會隨時間增長,很難知道有多少內存只是為了緩存而很好,但可以在不會對應用程序性能產生重大影響的情況下回收。這往往會導致對內存使用的誤解。

能夠"壓縮"內存是創建 CGroup v2 的主要原因之一。關於為什麼需要 CGroup V2 的更多歷史背景,請參閱 Chris Down 在 LISA21 上的 [演講](https://www.youtube.com/watch?v=kPMZYoRxtmg),他在其中介紹了無法正確設置最小內存是促使他創建 CGroup v2 和壓力停滯指標的原因之一。

幸運的是,Kubernetes 現在有了 `requests.memory` 下的 `memory.min` 和 `memory.high` 概念。這為我們提供了積極釋放此緩存內存供其他容器使用的選項。一旦容器達到內存高限制,內核就可以積極回收該容器的內存,直到達到設置的 `memory.min` 值。因此,當節點內存壓力加大時,我們有了更多的靈活性。

關鍵問題是將 `memory.min` 設置為何值?這就是內存壓力停滯指標發揮作用的地方。我們可以使用這些指標來檢測容器級別的內存"頻繁交換"。然後,我們可以使用諸如 [fbtax](https://facebookmicrosites.github.io/cgroup2/docs/fbtax-results.html) 之類的控制器通過查找此內存頻繁交換來檢測 `memory.min` 的正確值,並動態設置 `memory.min` 值為此設置。

### 總結
總而言之,很容易將以下概念混為一談:

* 利用率和飽和度
* Linux 性能規則與 Kubernetes 調度程序邏輯

必須格外小心將這些概念分開。性能和擴展能力有著深層次的聯繫。不必要的擴展會導致性能問題,而性能問題又會導致擴展問題。