//!!NODE_ROOT <section>
[."topic"]
[[aiml-observability,aiml-observability.title]]
= AI/ML on EKS - Observability
:info_doctype: section
:imagesdir: images/
:info_title: Observability
:info_abstract: Observability
:info_titleabbrev: Observability
:authors: ["Leah Tucker"]
:date: 2025-05-30

== Monitoring and Observability

== GPU Resource Optimization and Cost Management

=== Target high GPU utilization
Underutilized GPUs indicate that the allocated GPU resources are not being fully leveraged by the workloads, leading to wasted compute capacity. For AI/ML workloads on Amazon EKS, we recommend monitoring GPU utilization to target high GPU usage and optimize resource efficiency. Underutilized GPUs waste compute capacity and increase costs, while over-scheduling can lead to contention and performance degradation.

We recommend setting up https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/deploy-container-insights-EKS.html[Cloudwatch Container Insights on Amazon EKS] to identify specific pods, nodes, or workloads with low GPU utilization https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-metrics-enhanced-EKS.html[metrics]. It is easily integrated with Amazon EKS, enabling you to monitor GPU utilization and adjust pod scheduling or instance types if utilization falls below target levels. Alternatively, if this does not meet your specific requirements (e.g., advanced visualization), consider using NVIDIAâ€™s DCGM-Exporter alongside Prometheus and Grafana for Kubernetes-native monitoring. Both approaches provide insights into GPU metrics, enabling you to adjust pod scheduling or instance types if utilization falls below target levels. Check https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-NVIDIA-GPU.html[NVIDIA metrics] like `nvidia_smi_utilization_gpu` (GPU compute usage) and `nvidia_smi_utilization_memory` (GPU memory usage) via DCGM-Exporter or CloudWatch. Look for trends, such as consistently low utilization during certain hours or for specific jobs.

Static resource limits in Kubernetes (e.g., CPU, memory, and GPU counts) can lead to over-provisioning or underutilization, particularly for dynamic AI/ML workloads like inference. We recommend analyzing utilization trends and consolidate workloads onto fewer GPUs, ensuring each GPU is fully utilized before allocating new ones. If GPUs are underutilized, consider the following strategies to optimize scheduling and sharing. To learn more, see the https://quip-amazon.com/YU8bAEM6VS5h#temp:C:PFY3dfe711a3b474b23a755ef29a[Optimize GPU Resource Allocation with Node Affinity, Time-Slicing, MIG, and Fractional GPU Allocation] topic.
