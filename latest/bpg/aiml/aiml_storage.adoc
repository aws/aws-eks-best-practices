//!!NODE_ROOT <section>
[."topic"]
[[aiml-storage,aiml-storage.title]]
= AI/ML on EKS - Storage
:info_doctype: section
:imagesdir: images/
:info_title: Storage
:info_abstract: Storage
:info_titleabbrev: Storage
:authors: ["Leah Tucker"]
:date: 2025-05-30

== Data Management and Storage

=== Deploy AI Models to Pods Using a CSI Driver
AI/ML workloads often require access to large model artifacts (e.g., trained weights, configurations), and pods need a reliable, scalable way to access these without embedding them in container images, which can increase image sizes and Container registry pull times. To reduce operational overhead of managing volume mounts we recommend deploying AI models to pods by mounting Amazon storage services (e.g., S3, FSx for Lustre, EFS) as Persistent Volumes (PVs) using their respective CSI drivers. For implementation details, see subsequent topics in this section.

=== Optimize Storage for ML Model Caches on EKS
Leveraging an optimal storage solution is critical to minimize pod and application start-up latency, reduce memory usage, obtaining the desired levels of performance to accelerate workloads, and ensuring scalability of ML workloads. ML workloads often rely on model files (weights), which can be large and require shared access to data across pods or nodes. Selecting the optimal storage solution depends on your workload’s characteristics, such as single-node efficiency, multi-node access, latency requirements, cost constraints and also data integration requirements (such as with an Amazon S3 data repository). We recommend benchmarking different storage solutions with your workloads to understand which one meets your requirements, and we have provided the following options to help you evaluate based on your workload requirements.

The EKS CSI driver supports the following AWS Storage services, each have their own CSI driver and come with their own strengths for AI and ML workflows:

* https://docs.aws.amazon.com/eks/latest/userguide/s3-csi.html[Mountpoint for Amazon S3]
* https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi.html[Amazon FSx for Lustre]
* https://docs.aws.amazon.com/eks/latest/userguide/fsx-openzfs-csi.html[Amazon FSx for OpenZFS]
* https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html[Amazon EFS]
* https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html[Amazon EBS]

The choice of AWS Storage service depends on your deployment architecture, scale, performance requirements, and cost strategy. Storage CSI drivers need to be installed on your EKS cluster, which allows the CSI driver to create and manage Persistent Volumes (PV) outside the lifecycle of a Pod. Using the CSI driver, you can create PV definitions of supported AWS Storage services as EKS cluster resources. Pods can then access these storage volumes for their data volumes through creating a Persistent Volume Claim (PVC) for the PV. Depending on the AWS storage service and your deployment scenario, a single PVC (and its associated PV) can be attached to multiple Pods for a workload. For example, for ML training, shared training data is stored on a PV and accessed by multiple Pods; for real-time online inference, LLM models are cached on a PV and accessed by multiple Pods. Sample PV and PVC YAML files for AWS Storage services are provided below to help you get started.

**Scenario: Multiple GPU instances workload**

**Amazon FSx for Lustre**: In scenarios where you have **multiple EC2 GPU compute instance** environment with latency-sensitive and high-bandwidth throughput dynamic workloads, such as distributed training and model serving, and you require native Amazon S3 data repository integration, we recommend https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html[Amazon FSx for Lustre]. FSx for Lustre provides a fully managed high performance parallel filesystem that is designed for compute-intensive workloads like high-performance computing (HPC), Machine Learning.

You can https://docs.aws.amazon.com/eks/latest/userguide/fsx-csi.html[Install the FSx for Lustre CSI driver] to mount FSx filesystems on EKS as a Persistent Volume (PV), then deploy FSx for Lustre file system as a standalone high performance cache or as an S3-linked file system to act as a high performance cache for S3 data, providing fast I/O and high throughput for data access across your GPU compute instances. FSx for Lustre can be deployed with either Scratch-SSD or Persistent-SSD storage options:

* **Scratch-SSD storage**: Recommended for workloads that are ephemeral or short-lived (hours), with fixed throughput capacity per-TiB provisioned.
* **Persistent-SSD storage**: Recommended for mission-critical, long-running workloads that require the highest level of availability, for example HPC simulations, big data analytics or Machine Learning training. With Persistent-SSD storage, you can configure both the storage capacity and throughput capacity (per-TiB) that is required.

Performance considerations:

* **Administrative pod to manage FSx for Lustre file system**: Configure an "administrative" Pod that has the lustre client installed and has the FSx file system mounted. This will enable an access point to enable fine-tuning of the FSx file system, and also in situations where you need to pre-warm the FSx file system with your ML training data or LLM models before starting up your GPU compute instances. This is especially important if your architecture utilizes Spot-based Amazon EC2 GPU/compute instances, where you can utilize the administrative Pod to "warm" or "pre-load" desired data into the FSx file system, so that the data is ready to be processed when you run your Spot based Amazon EC2 instances.
* **Elastic Fabric Adapter (EFA)**: Persistent-SSD storage deployment types support https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html[Elastic Fabric Adapter (EFA)], where using EFA is ideal for high performance and throughput-based GPU-based workloads. Note that FSx for Lustre supports NVIDIA GPUDirect Storage (GDS), where GDS is a technology that creates a direct data path between local or remote storage and GPU memory, to enable faster data access.
* **Compression**: Enable data compression on the file system if you have file types that can be compressed. This can help to increase performance as data compression reduces the amount of data that is transferred between FSx for Lustre file servers and storage.
* **Lustre file system striping configuration**:
 ** **Data striping**: Allows FSx for Luster to distribute a file’s data across multiple Object Storage Targets (OSTs) within a Lustre file system maximizes parallel access and throughput, especially for large-scale ML training jobs.
 ** **Standalone file system striping**: By default, a 4-component Lustre striping configuration is created for you via the https://docs.aws.amazon.com/fsx/latest/LustreGuide/performance.html#striping-pfl[Progressive file layouts (PFL)] capability of FSx for Lustre. In most scenarios you don't need to update the default PFL Lustre stripe count/size. If you need to adjust the Lustre data striping, then you can manually adjust the Lustre striping by referring to https://docs.aws.amazon.com/fsx/latest/LustreGuide/performance.html#striping-data[striping parameters of a FSx for Lustre file system].
 ** **S3-Linked File system**:  Files imported into the FSx file system using the native Amazon S3 integration (Data Repository Association or DRA) don't use the default PFL layout, but instead use the layout in the file system's `ImportedFileChunkSize` parameter. S3-imported files larger than the `ImportedFileChunkSize` will be stored on multiple OSTs with a stripe count based on the `ImportedFileChunkSize` defined value (default 1GiB). If you have large files, we recommend tuning this parameter to a higher value.
 ** **Placement**: Deploy an FSx for Lustre file system in the same Availability Zone as your compute or GPU nodes to enable the lowest latency access to the data, avoid cross Availability Zone access access patterns. If you have multiple GPU nodes located in different Availability Zones, then we recommend deploying a FSx file system in each Availability Zone for low latency data access.

**Example**

Persistent Volume (PV) definition for an FSx for Lustre file system, using Static Provisioning (where the FSx instance has already been provisioned).

[,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: fsx-pv
spec:
  capacity:
    storage: 1200Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  mountOptions:
    - flock
  persistentVolumeReclaimPolicy: Recycle
  csi:
    driver: fsx.csi.aws.com
    volumeHandle: [FileSystemId of FSx instance]
    volumeAttributes:
      dnsname: [DNSName of FSx instance]
      mountname: [MountName of FSx instance]
----

**Example**

Persistent Volume Claim definition for PV called `fsx-pv`:

[,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fsx-claim
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: ""
  resources:
    requests:
      storage: 1200Gi
  volumeName: fsx-pv
----

**Example**

Configure a pod to use an Persistent Volume Claim of `fsx-claim`:

[,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: fsx-app
spec:
  containers:
  - name: app
    image: amazonlinux:2023
    command: ["/bin/sh"]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: fsx-claim

----

For complete examples, see the https://github.com/kubernetes-sigs/aws-fsx-csi-driver/tree/master/examples/kubernetes[FSx for Lustre Driver Examples in GitHub].

**Scenario: Single GPU instance workload**

**Mountpoint for Amazon S3 with CSI Driver:** You can mount an S3 bucket as a volume in your pods using https://docs.aws.amazon.com/eks/latest/userguide/s3-csi.html[Mountpoint for Amazon S3 CSI driver]. This method allows for fine-grained access control over which Pods can access specific S3 buckets. Each pod has its own mountpoint instance and local cache (5-10GB), isolating model loading and read performance between pods. This setup supports pod-level authentication with IAM Roles for Service Accounts (IRSA) and independent model versioning for different models or customers. The trade-off is increased memory usage and API traffic, as each pod issues S3 API calls and maintains its own cache.

**Example**
Partial example of a Pod deployment YAML with CSI Driver:

```yaml
# CSI driver dynamically mounts the S3 bucket for each pod

volumes:
  - name: s3-mount
    csi:
      driver: s3.csi.aws.com
      volumeAttributes:
        bucketName: your-s3-bucket-name
        mountOptions: "--allow-delete"  # Optional
        region: us-west-2

containers:
  - name: inference
    image: your-inference-image
    volumeMounts:
      - mountPath: /models
        name: s3-mount
volumeMounts:
  - name: model-cache
    mountPath: /models
volumes:
  - name: model-cache
    hostPath:
      path: /mnt/s3-model-cache

```
**Performance considerations:**

* **Data caching**: Mountpoint for S3 can cache content to reduce costs and improve performance for repeated reads to the same file. Refer to https://github.com/awslabs/mountpoint-s3/blob/main/doc/CONFIGURATION.md#caching-configuration[Caching configuration] for caching options and parameters.
* **Object part-size**: When storing and accessing files over 72GB in size, refer to https://github.com/awslabs/mountpoint-s3/blob/main/doc/CONFIGURATION.md#configuring-mountpoint-performance[Configuring Mountpoint performance] to understand how to configure the  `--read-part-size` and `--write-part-size` command-line parameters to meet your data profile and workload requirements.
* **https://github.com/awslabs/mountpoint-s3/blob/main/doc/CONFIGURATION.md#shared-cache[Shared-cache]** is designed for objects up to 1MB in size. It does not support large objects. Use the https://github.com/awslabs/mountpoint-s3/blob/main/doc/CONFIGURATION.md#local-cache[Local cache] option for caching objects in NVMe or EBS volumes on the EKS node.
* **API request charges**: When performing a high number of file operations with the Mountpoint for S3, API request charges can become a portion of storage costs. To mitigate this, if strong consistency is not required, always enable metadata caching and set  the `metadata-ttl` period to reduce the number of API operations to S3.

For more details, see the https://docs.aws.amazon.com/eks/latest/userguide/s3-csi.html[Mountpoint for Amazon S3 CSI Driver] in the Amazon EKS official documentation. We recommend monitoring the performance metrics of https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudwatch-monitoring.html[Amazon S3 with Amazon CloudWatch metrics] if bottlenecks occur and adjusting your configuration where required.

=== Amazon EFS for shared model caches

In scenarios where you have a **multiple EC2 GPU compute instance environment** and have dynamic workloads requiring shared model access across multiple nodes and Availability Zones (e.g., real-time online inference with Karpenter) with moderate performance and scalability needs, we recommend using an Amazon Elastic File System (EFS) file system as a Persistent Volume through the EFS CSI Driver. https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html[Amazon EFS] is a fully managed, highly available, and scalable cloud-based NFS file system that enables EC2 instances and containers with shared file storage,  with consistent performance, and where no upfront provisioning of storage is required. Use EFS as the model volume, and mount the volume as a shared filesystem through defining a Persistent Volume on the EKS cluster. Each Persistent Volume Claim (PVC) that is backed by an EFS file system is created as an https://docs.aws.amazon.com/efs/latest/ug/efs-access-points.html[EFS Access-point to the EFS file system]. EFS allows multiple nodes and pods to access the same model files, eliminating the need to sync data to each node’s filesystem. https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html[Install the EFS CSI driver] to integrate EFS with EKS.

You can deploy an Amazon EFS file system with the following throughput modes:

* **Bursting Throughput**: Scales throughput with file system size, suitable for varying workloads with occasional bursts.
* **Provisioned Throughput**: Dedicated throughput, ideal for consistent ML training jobs with predictable performance needs within limits.
* **Elastic Throughput (recommended for ML)**: Automatically scales based on workload, cost-effectiveness for varying ML workloads.

To view performance specifications, see https://docs.aws.amazon.com/efs/latest/ug/performance.html[Amazon EFS performance specifications].

**Performance considerations**:

* Use Elastic Throughput for varying workloads.
* Use Standard storage class for active ML workloads.

For complete examples of using Amazon EFS file system as a persistent Volume within your EKS cluster and Pods, refer to the https://github.com/kubernetes-sigs/aws-efs-csi-driver/tree/master/examples/kubernetes[EFS CSI Driver Examples in GitHub].

**Monitoring performance**
Poor disk performance can delay container image reads, increase pod startup latency, and degrade inference or training throughput. We recommend the following methods to monitor the performance metrics of the respective AWS Storage service if bottlenecks occur and adjusting your configuration where required.

* https://docs.aws.amazon.com/fsx/latest/LustreGuide/monitoring-cloudwatch.html[Amazon FSx console and its performance metrics] to view the performance metrics related to your FSx file system.
* https://docs.aws.amazon.com/efs/latest/ug/accessingmetrics.html[Access Amazon CloudWatch metrics for Amazon EFS] to view the performance metrics related to your EFS file system.
* https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudwatch-monitoring.html[Monitoring Amazon S3 metrics with Amazon CloudWatch] to view performance details related to your S3 bucket.
