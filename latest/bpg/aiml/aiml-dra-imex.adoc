[.topic]
[#aiml-dra-imex]
= Optimize GPU workloads with IMEX using GB200 P6e instances
:info_titleabbrev: IMEX

IMEX (Internode Memory Exchange) enables memory-coherent communication
across nodes for distributed training on NVIDIA GB200 UltraClusters.

Do the following steps.

. Define a `ComputeDomain` for multi-node training with a file named
`imex-compute-domain.yaml`:
+
[source,yaml,subs="verbatim,attributes"]
----
apiVersion: resource.nvidia.com/v1beta1
kind: ComputeDomain
metadata:
  name: distributed-training-domain
  namespace: default
spec:
  numNodes: 2
  channel:
    resourceClaimTemplate:
      name: imex-channel-template
----

. Define a Pod using IMEX channels with a file named `imex-pod.yaml`:
+
[source,yaml,subs="verbatim,attributes"]
----
apiVersion: v1
kind: Pod
metadata:
  name: imex-distributed-training
  namespace: default
  labels:
    app: imex-training
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.clique
            operator: Exists
  containers:
  - name: distributed-training
    image: nvcr.io/nvidia/pytorch:25.04-py3
    command: ["bash", "-c"]
    args:
    - |
      echo "=== IMEX Channel Verification ==="
      ls -la /dev/nvidia-caps-imex-channels/
      echo ""
      
      echo "=== GPU Information ==="
      nvidia-smi
      echo ""
      
      echo "=== NCCL Test (if available) ==="
      python -c "
      import torch
      import torch.distributed as dist
      import os
      
      print(f'CUDA available: {torch.cuda.is_available()}')
      print(f'CUDA device count: {torch.cuda.device_count()}')
      
      if torch.cuda.is_available():
          for i in range(torch.cuda.device_count()):
              print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
      
      # Check for IMEX environment variables
      imex_vars = [k for k in os.environ.keys() if 'IMEX' in k or 'NVLINK' in k]
      if imex_vars:
          print('IMEX Environment Variables:')
          for var in imex_vars:
              print(f'  {var}={os.environ[var]}')
      
      print('IMEX channel verification completed')
      "
      
      # Keep container running for inspection
      sleep 3600
    resources:
      claims:
      - name: imex-channel-0
      - name: imex-channel-1
  resourceClaims:
  - name: imex-channel-0
    resourceClaimTemplateName: imex-channel-template
  - name: imex-channel-1
    resourceClaimTemplateName: imex-channel-template
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
----
+
NOTE: This requires P6e GB200 instances.

. Deploy IMEX by applying the `ComputeDomain` and templates:
+
[source,bash,subs="verbatim,attributes"]
----
kubectl apply -f imex-claim-template.yaml
kubectl apply -f imex-compute-domain.yaml
kubectl apply -f imex-pod.yaml
----

. Check the `ComputeDomain` status.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get computedomain distributed-training-domain
----

. Monitor the IMEX daemon deployment.
+
[source,bash,subs="verbatim,attributes"]
----
kubectl get pods -n nvidia-dra-driver -l [resource.nvidia.com/computeDomain](http://resource.nvidia.com/computeDomain)
----

. Check the IMEX channels in the Pod:
+
[source,bash,subs="verbatim,attributes"]
----
kubectl exec imex-distributed-training -- ls -la /dev/nvidia-caps-imex-channels/
----

. View the Pod logs:
+
[source,bash,subs="verbatim,attributes"]
----
kubectl logs imex-distributed-training
----
+
The following is an example of expected output:
+
[source,bash,subs="verbatim,attributes",role="nocopy"]
----
=== IMEX Channel Verification ===
total 0
drwxr-xr-x. 2 root root 80 Jul 8 10:45 .
drwxr-xr-x. 6 root root 380 Jul 8 10:45 ..
crw-rw-rw-. 1 root root 241, 0 Jul 8 10:45 channel0
crw-rw-rw-. 1 root root 241, 1 Jul 8 10:45 channel1
----

For more information, see the
link:https://github.com/NVIDIA/k8s-dra-driver-gpu/discussions/249[NVIDIA
example] on GitHub.
