[.topic]
[[hybrid-nodes-kubernetes-pod-failover,hybrid-nodes-kubernetes-pod-failover.title]]
= Kubernetes pod failover through network disconnections
:info_doctype: section
:info_title: Kubernetes pod failover through network disconnections
:info_titleabbrev: Kubernetes pod failover
:info_abstract: Kubernetes pod failover through network disconnections

We start with a review of the key concepts, components, and settings that play a role in how Kubernetes behaves during network disconnections between nodes and the Kubernetes control plane. EKS is upstream Kubernetes conformant, so all of the Kubernetes concepts, components, and settings detailed in this section apply to EKS and EKS Hybrid Nodes deployments.

== Concepts

[.underline]#Taints and Tolerations#: Taints and tolerations are used in Kubernetes to control the scheduling of pods onto nodes. Taints are used by the node-lifecycle-controller to indicate nodes that are not eligible for scheduling and nodes that should evict the pods running on them. When nodes are unreachable due to a network disconnection, the node.kubernetes.io/unreachable taint is applied by the node-lifecycle-controller with a NoSchedule effect and with a NoExecute effect if certain conditions are met. The node.kubernetes.io/unreachable taint applied by the node-lifecycle-controller corresponds to the NodeCondition Ready being Unknown. Tolerations for taints can be specified by users at the application level in the PodSpec.

* NoSchedule: No new Pods will be scheduled on the tainted node unless they have a matching toleration. Pods currently running on the node are not evicted.
* NoExecute: Pods that do not tolerate the taint are evicted immediately. Pods that tolerate the taint without specifying tolerationSeconds in their toleration specification remain bound forever. Pods that tolerate the taint with a specified tolerationSeconds remain bound for the specified amount of time. After that time elapses, the node lifecycle controller evicts the Pods from the node.

[.underline]#Node Leases#: Kubernetes uses the Lease API to communicate kubelet node heartbeats to the Kubernetes API server. For every node, there is a Lease object with a matching name. Under the hood, every kubelet heartbeat is an update request to this Lease object, updating the spec.renewTime field for the Lease. The Kubernetes control plane uses the timestamp of this field to determine node availability. In the case of network disconnections between nodes and the Kubernetes control plane, nodes are unable to update the spec.renewTime of their leases, which is interpreted to signal that the NodeCondition Ready is Unknown by the Kubernetes control plane.

== Components

image::images/hybrid/k8s-components-pod-failover.png[Kubernetes components involved in pod failover behavior,scaledwidth=100%]

[cols="2,2,5"]
|===
|Component|Sub-component|Description

|Kubernetes control plane
|kube-api-server
|The API server is a component of the Kubernetes control plane that exposes the Kubernetes API.

|Kubernetes control plane
|node-lifecycle-controller
|One of the controllers that the kube-controller-manager runs. Responsible for noticing and responding to issues with nodes.

|Kubernetes control plane
|kube-scheduler
|Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.

|Kubernetes nodes
|kubelet
|An agent that runs on each node in the cluster. The kubelet takes a set of PodSpecs and ensures the containers described in those PodSpecs are running and healthy. 
|===

== Configuration settings

[cols="1,2,5,1,1,1"]
|===
|Component|Setting|Description|K8s default|EKS default|Configurable in EKS

|kube-api-server
|default-unreachable-toleration-seconds
|Indicates the `tolerationSeconds` of the toleration for `unreachable:NoExecute` that is added by default to every pod that does not already have such a toleration.
|300
|300
|No

|node-lifecycle-controller
|node-monitor-grace-period
|Amount of time a node can be unresponsive before marking it unhealthy. Must be N times more than kubelet's `nodeStatusUpdateFrequency`, where N means number of retries allowed for kubelet to post node status.
|40
|40
|No

|node-lifecycle-controller
|large-cluster-size-threshold
|Number of nodes from which node-lifecycle-controller treats the cluster as large for the eviction logic purposes. `--secondary-node-eviction-rate` is overridden to 0 for clusters this size or smaller.
|50
|100,000
|No

|node-lifecycle-controller
|unhealthy-zone-threshold
|Percentage of nodes in a zone which need to be Not Ready for a zone to be treated as unhealthy.
|55%
|55%
|No

|kubelet
|node-status-update-frequency
|How often kubelet posts node status to control plane. Be cautious when changing the constant, it must work with `nodeMonitorGracePeriod` in node-lifecycle-controller.
|10
|10
|Yes

|kubelet
|node-labels
|Labels to add when registering the node in the cluster. The label `topology.kubernetes.io/zone` can be specified with hybrid nodes to group nodes into zones.
|None
|None
|Yes
|===

== Kubernetes pod failover through network disconnections

The behavior described in this section assumes pods are running as Kubernetes Deployments with the default settings. The behavior described in this section assumes EKS is used as the Kubernetes provider. Please note, the actual behavior may vary based on the environment, nature of the network disconnection, applications, dependencies, and cluster configuration. The behavior described in this guide was validated using a specific application, cluster configuration, and subset of plugins. It is strongly recommended to test the behavior with your own applications and environments before deploying to production.

When there are network disconnections between nodes and the Kubernetes control plane, the kubelet running the nodes cannot communicate with the Kubernetes control plane. Because of this, the kubelet cannot take action to evict pods on the nodes until the connection with the Kubernetes control plane is restored. This means that pods running on nodes before a network disconnection continue running during the disconnection, assuming there are no other failures that cause the pods to shutdown during the disconnection. In summary, you can achieve static stability during network disconnections between nodes and the Kubernetes control plane but you cannot perform mutating operations on your nodes or workloads during network disconnections.

There are four scenarios that result in different pod failover behaviors based on the nature of the network disconnection. For all scenarios, we observed that the overall health of the cluster is automatically restored without operator intervention once the nodes reconnect to the Kubernetes control plane. The scenarios below contain expected results based on the observations of our testing but these expected results may not apply to all possible application and cluster configurations.

=== Scenario 1: Full disruption

*Expected result*: Pods running on unreachable nodes are not evicted and continue running on the same nodes.

Full disruption means all nodes in the cluster are disconnected from the Kubernetes control plane. In this scenario, the node-lifecycle-controller running on the Kubernetes control plane detects that all nodes in the cluster are unreachable and cancels pod evictions.

Cluster administrators will see all nodes with status `Unknown` during the disconnection. The pod status will not change and new pods will not be scheduled on other nodes during the disconnection and through reconnection. 

=== Scenario 2: Majority zone disruption

*Expected Result*: Pods running on unreachable nodes are not evicted and continue running on the same nodes.

Majority zone disruption means the majority of nodes in a zone are disconnected from the Kubernetes control plane. Zones in Kubernetes are defined by nodes with the same `topology.kubernetes.io/zone` label. If no zones are defined in the cluster then a majority means the majority of nodes in the cluster are disconnected. Majority is defined by the `unhealthy-zone-threshold` setting of the node-lifecycle-controller and is set to 55% by default in Kubernetes and in EKS. In this scenario, the node-lifecycle-controller considers the number of unreachable nodes as well as the number of nodes in the entire cluster to determine whether to evict pods from nodes. In the case of EKS, because `large-cluster-size-threshold` is set to 100,000, if 55% or more of nodes in a zone are unreachable, all pod evictions are cancelled because most clusters are smaller than 100,000 nodes.

Cluster administrators will see a majority of nodes in the zone with status `Not Ready` during the disconnection, but the pod status will not change and pods will not be rescheduled on other nodes during the disconnection and through reconnection. 

Note, the behavior described above only applies for clusters larger than 3 nodes. In the case of clusters equal to or smaller than 3 nodes, pods on unreachable nodes will be scheduled for eviction and new pods will be scheduled on healthy nodes.

Note, during testing iterations, it was observed that occasionally pods are evicted from 1, and only 1, of the unreachable nodes during network disconnections when a majority of nodes in a zone are unreachable. We are continuing to track down our suspicions of a race condition in the Kubernetes node-lifecycle-controller leading to this behavior.

=== Scenario 3: Minority disruption

*Expected Result*: Pods are evicted from unreachable nodes and new pods are scheduled on available, eligible nodes.

Minority disruption means the minority of nodes in a zone are disconnected from the Kubernetes control plane. If no zones are defined in the cluster then a minority disruption means the minority of nodes in the cluster are disconnected. Minority is defined by the `unhealthy-zone-threshold` setting of the node-lifecycle-controller and is set to 55% by default in Kubernetes and in EKS. In this scenario, if the network disconnection lasts longer than 5 minutes (`default-unreachable-toleration-seconds`) and 40 seconds (`node-monitor-grace-period`), and less than 55% of nodes in a zone are unreachable, new pods will be scheduled on healthy nodes and the pods on the unreachable nodes in the zone will be scheduled for eviction. 

Cluster administrators will see new pods created on healthy nodes and the pods on the disconnected nodes will have status `Terminating`. As a reminder, even though the pods on the disconnected nodes have status `Terminating`, they will not be evicted from the node until the node reconnects to the Kubernetes control plane.

=== Scenario 4: Node restart during network disruption

*Expected Result*: Pods running on unreachable nodes are not started until the unreachable nodes reconnect to the Kubernetes control plane. The pod failover behavior is subject to the nature of the network disconnection as outlined in Scenarios 1-3.

Node restart during network disruption means that there were multiple simultaneous failures, a network disconnection and then another event that caused the kubelet to restart such as a power cycle, out-of-memory error, or similar issues. In this scenario, the pods that were running on the node when the network disconnection happened will not be automatically restarted throughout the network disconnection if the kubelet was restarted. The reason for this is the kubelet contacts the Kubernetes API server during startup to learn which pods it should run. When the kubelet cannot contact the Kubernetes API server, as is the case during a network disconnection, it cannot get the information it needs to start the pods.

In this scenario, local troubleshooting tools such as crictl cannot be used to manually start pods in a break-glass effort because Kubernetes follows a pattern of removing the failed pods and creating new pods rather than restarting existing pods (see https://github.com/containerd/containerd/pull/10213[#10213] in the containerd GitHub repo for more information). Static pods are the only Kubernetes workload object that are controlled by the kubelet and can be restarted during these scenarios. It is generally not recommended to use static pods for application deployments, and instead you should deploy multiple replicas across different hosts to maintain application availability in the event of an additional simultaneous failures during a network disconnection between your nodes and the Kubernetes control plane.